{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerospike Connect for Spark Tutorial for Python\n",
    "#### Tested with Spark connector 3.4.1, ASDB EE 6.0.0.0 EE, Java 8, Apache Spark 3.1.2, Python 3.7  and Scala 2.12.11 and [Spylon]( https://pypi.org/project/spylon-kernel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ensure Database Is Running\n",
    "This notebook requires that Aerospike Database is running."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!asd >& /dev/null\n",
    "!pgrep -x asd >/dev/null && echo \"Aerospike database is running!\" || echo \"**Aerospike database is not running!**\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set Aerospike, Spark, and Spark Connector Paths and Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP Address or DNS name for one host in your Aerospike cluster\n",
    "import socket\n",
    "AS_HOST =socket.gethostname()\n",
    "# Name of one of your namespaces. Type 'show namespaces' at the aql prompt if you are not sure\n",
    "AS_NAMESPACE = \"test\" \n",
    "AS_PORT = 3000 # Usually 3000, but change here if not\n",
    "AS_CONNECTION_STRING = AS_HOST + \":\"+ str(AS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aerospike Spark Connector settings when running in Binder\n",
    "import os\n",
    "# PATH to aerospike spark jar file (If running locally, set path local aerospike client jar. SPARK_HOME is not needed)\n",
    "SPARK_HOME = \"/opt/spark-nb/spark-dir-link\"\n",
    "AEROSPIKE_JAR_PATH =\"/opt/spark-nb/aerospike-jar-link\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars ' + AEROSPIKE_JAR_PATH + ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Alternative Setup for Running Notebook in Different Environment\n",
    "Please follow the instructions below **instead of the setup above** if you are running this notebook in a different environment from the one provided by the Aerospike Intro-Notebooks container.\n",
    "```\n",
    "# IP Address or DNS name for one host in your Aerospike cluster\n",
    "AS_HOST = \"<seed-host-ip>\"\n",
    "# Name of one of your namespaces. Type 'show namespaces' at the aql prompt if you are not sure\n",
    "AS_NAMESPACE = \"<namespace>\" \n",
    "AEROSPIKE_SPARK_JAR_VERSION=\"<spark-connector-version>\"\n",
    "AS_PORT = 3000 # Usually 3000, but change here if not\n",
    "AS_CONNECTION_STRING = AS_HOST + \":\"+ str(AS_PORT)\n",
    "\n",
    "# Set SPARK_HOME path.\n",
    "SPARK_HOME = '<spark-home-dir>'\n",
    "\n",
    "# Please download the appropriate Aeropsike Connect for Spark from the [download page](https://enterprise.aerospike.com/enterprise/download/connectors/aerospike-spark/notes.html)  \n",
    "# Set `AEROSPIKE_JAR_PATH` with path to the downloaded binary\n",
    "import os \n",
    "AEROSPIKE_JAR_PATH= \"<aerospike-jar-dir>/aerospike-spark-assembly-\"+AEROSPIKE_SPARK_JAR_VERSION+\".jar\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars ' + AEROSPIKE_JAR_PATH + ' pyspark-shell'\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we locate the Spark installation - this will be found using the SPARK_HOME environment variable that you will have set \n",
    "\n",
    "import findspark\n",
    "# findspark.init()\n",
    "# Initialize findspark with SPARK_HOME path only when running in binder. If running locally, make sure virtualenv with findspark is activated.\n",
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Aerospike properties in the Spark Session object. Please visit [Configuring Aerospike Connect for Spark](https://docs.aerospike.com/docs/connect/processing/spark/configuration.html) for more information about the properties used on this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "conf=sc._conf.setAll([(\"aerospike.namespace\",AS_NAMESPACE),\n",
    "                      (\"aerospike.seedhost\",AS_CONNECTION_STRING)])\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf.setMaster(\"local[1]\"))\n",
    "spark = SparkSession(sc)\n",
    "spark\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema in the Spark Connector\n",
    "\n",
    "-  Aerospike is schemaless, however Spark adher to schema. After the schema is decided upon (either through inference or given), data within the bins must honor the types. \n",
    "\n",
    "- To infer schema, the connector samples a set of records (configurable through `aerospike.schema.scan`) to decide the name of bins/columns and their types. This implies that the derived schema depends entirely upon sampled records.  \n",
    "\n",
    "- **Note that `__key` was not part of provided schema. So how can one query using `__key`? We can just add `__key` in provided schema with appropriate type. Similarly we can add `__gen` or `__ttl` etc.**  \n",
    "         \n",
    "      schemaWithPK =  StructType([\n",
    "                StructField(\"__key\",IntegerType(), False),    \n",
    "                StructField(\"id\", IntegerType(), False),\n",
    "                StructField(\"name\", StringType(), False),\n",
    "                StructField(\"age\", IntegerType(), False),\n",
    "                StructField(\"salary\",IntegerType(), False)])\n",
    "                \n",
    "- **We recommend that you provide schema for queries that involve [collection data types](https://docs.aerospike.com/docs/guide/cdt.html) such as lists, maps, and mixed types. Using schema inference for CDT may cause unexpected issues.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexible schema inference \n",
    "Spark assumes that the underlying data store (Aerospike in this case) follows a strict schema for all the records within a table. However, Aerospike is a No-SQL DB and is schemaless. For further information on the Spark connector reconciles those differences, visit [Flexible schema](https://docs.aerospike.com/docs/connect/processing/spark/configuration.html#flexible-schemas) page\n",
    " - aerospike.schema.flexible = true (default)\n",
    " - aerospike.schema.flexible = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_records=200\n",
    "\n",
    "schema = StructType( \n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "inputBuf = []\n",
    "for  i in range(1, num_records) :\n",
    "         name = \"name\"  + str(i)\n",
    "         id_ = i \n",
    "         inputBuf.append((id_, name))\n",
    "    \n",
    "inputRDD = spark.sparkContext.parallelize(inputBuf)\n",
    "inputDF=spark.createDataFrame(inputRDD,schema)\n",
    "\n",
    "#Write the Sample Data to Aerospike\n",
    "inputDF \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".format(\"aerospike\")  \\\n",
    ".option(\"aerospike.writeset\", \"py_input_data\")\\\n",
    ".option(\"aerospike.updateByKey\", \"id\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aerospike.schema.flexible = true (default) \n",
    "   \n",
    "  If none of the column types in the user-specified schema match the bin types of a record in Aerospike, a record with NULLs is returned in the result set. \n",
    "\n",
    "Please use the filter() in Spark to filter out NULL records. For e.g. df.filter(\"gender == NULL\").show(false), where df is a dataframe and gender is a field that was not specified in the user-specified schema. \n",
    "\n",
    "If the above mismatch is limited to fewer columns in the user-specified schema then NULL would be returned for those columns in the result set. **Note: there is no way to tell apart a NULL due to missing value in the original data set and the NULL due to mismatch, at this point. Hence, the user would have to treat all NULLs as missing values.** The columns that are not a part of the schema will be automatically filtered out in the result set by the connector.\n",
    "\n",
    "Please note that if any field is set to NOT nullable i.e. nullable = false, your query will error out if there’s a type mismatch between an Aerospike bin and the column type specified in the user-specified schema.\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaIncorrect = StructType( \n",
    "    [\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", IntegerType(), True)  ##Note incorrect type of name bin\n",
    "    ]\n",
    ")\n",
    "\n",
    "flexSchemaInference=spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".schema(schemaIncorrect) \\\n",
    ".option(\"aerospike.set\", \"py_input_data\").load()\n",
    "\n",
    "flexSchemaInference.show(5)\n",
    "\n",
    "##notice all the contents of name column is null due to schema mismatch and aerospike.schema.flexible = true (by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aerospike.schema.flexible = false \n",
    "\n",
    "If a mismatch between the user-specified schema and the schema of a record in Aerospike is detected at the bin/column level, your query will error out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When strict matching is set, we will get an exception due to type mismatch with schema provided.\n",
    "\n",
    "try:\n",
    "    errorDFStrictSchemaInference=spark \\\n",
    "    .read \\\n",
    "    .format(\"aerospike\") \\\n",
    "    .schema(schemaIncorrect) \\\n",
    "    .option(\"aerospike.schema.flexible\" ,\"false\") \\\n",
    "    .option(\"aerospike.set\", \"py_input_data\").load()\n",
    "    errorDFStrictSchemaInference.show(5)\n",
    "except Exception as e:    \n",
    "    pass\n",
    "     \n",
    "#This will throw error due to type mismatch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create age vs salary data, using three different Gaussian distributions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Make sure we get the same results every time this workbook is run\n",
    "# Otherwise we are occasionally exposed to results not working out as expected\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Create covariance matrix from std devs + correlation\n",
    "def covariance_matrix(std_dev_1,std_dev_2,correlation):\n",
    "    return [[std_dev_1 ** 2, correlation * std_dev_1 * std_dev_2], \n",
    "           [correlation * std_dev_1 * std_dev_2, std_dev_2 ** 2]]\n",
    "\n",
    "# Return a bivariate sample given means/std dev/correlation\n",
    "def age_salary_sample(distribution_params,sample_size):\n",
    "    mean = [distribution_params[\"age_mean\"], distribution_params[\"salary_mean\"]]\n",
    "    cov = covariance_matrix(distribution_params[\"age_std_dev\"],distribution_params[\"salary_std_dev\"],\n",
    "                            distribution_params[\"age_salary_correlation\"])\n",
    "    return np.random.multivariate_normal(mean, cov, sample_size).T\n",
    "\n",
    "# Define the characteristics of our age/salary distribution\n",
    "age_salary_distribution_1 = {\"age_mean\":25,\"salary_mean\":50000,\n",
    "                             \"age_std_dev\":1,\"salary_std_dev\":5000,\"age_salary_correlation\":0.3}\n",
    "\n",
    "age_salary_distribution_2 = {\"age_mean\":45,\"salary_mean\":80000,\n",
    "                             \"age_std_dev\":4,\"salary_std_dev\":8000,\"age_salary_correlation\":0.7}\n",
    "\n",
    "age_salary_distribution_3 = {\"age_mean\":35,\"salary_mean\":70000,\n",
    "                             \"age_std_dev\":2,\"salary_std_dev\":9000,\"age_salary_correlation\":0.1}\n",
    "\n",
    "distribution_data = [age_salary_distribution_1,age_salary_distribution_2,age_salary_distribution_3]\n",
    "\n",
    "# Sample age/salary data for each distributions\n",
    "sample_size_1 = 100;\n",
    "sample_size_2 = 120;\n",
    "sample_size_3 = 80;\n",
    "sample_sizes = [sample_size_1,sample_size_2,sample_size_3]\n",
    "group_1_ages,group_1_salaries = age_salary_sample(age_salary_distribution_1,sample_size=sample_size_1)\n",
    "group_2_ages,group_2_salaries = age_salary_sample(age_salary_distribution_2,sample_size=sample_size_2)\n",
    "group_3_ages,group_3_salaries = age_salary_sample(age_salary_distribution_3,sample_size=sample_size_3)\n",
    "\n",
    "ages=np.concatenate([group_1_ages,group_2_ages,group_3_ages])\n",
    "salaries=np.concatenate([group_1_salaries,group_2_salaries,group_3_salaries])\n",
    "\n",
    "print(\"Data created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display simulated age/salary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sample data\n",
    "group_1_colour, group_2_colour, group_3_colour ='red','blue', 'pink'\n",
    "plt.xlabel('Age',fontsize=10)\n",
    "plt.ylabel(\"Salary\",fontsize=10) \n",
    "\n",
    "plt.scatter(group_1_ages,group_1_salaries,c=group_1_colour,label=\"Group 1\")\n",
    "plt.scatter(group_2_ages,group_2_salaries,c=group_2_colour,label=\"Group 2\")\n",
    "plt.scatter(group_3_ages,group_3_salaries,c=group_3_colour,label=\"Group 3\")\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to Aerospike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the above records into a Data Frame\n",
    "# First of all, create an array of arrays\n",
    "inputBuf = []\n",
    "\n",
    "for  i in range(0, len(ages)) :\n",
    "     id = i + 1 # Avoid counting from zero\n",
    "     name = \"Individual: {:03d}\".format(id)\n",
    "     # Note we need to make sure values are typed correctly\n",
    "     # salary will have type numpy.float64 - if it is not cast as below, an error will be thrown\n",
    "     age = float(ages[i])\n",
    "     salary = int(salaries[i])\n",
    "     inputBuf.append((id, name,age,salary))\n",
    "\n",
    "# Convert to an RDD \n",
    "inputRDD = spark.sparkContext.parallelize(inputBuf)\n",
    "       \n",
    "# Convert to a data frame using a schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", DoubleType(), True),\n",
    "    StructField(\"salary\",IntegerType(), True)\n",
    "])\n",
    "\n",
    "inputDF=spark.createDataFrame(inputRDD,schema)\n",
    "\n",
    "#Write the data frame to Aerospike, the id field is used as the primary key\n",
    "inputDF \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".format(\"aerospike\")  \\\n",
    ".option(\"aerospike.set\", \"salary_data\")\\\n",
    ".option(\"aerospike.updateByKey\", \"id\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Spark SQL syntax to insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aerospike DB needs a Primary key for record insertion. Hence, you must identify the primary key column \n",
    "#using for example .option(“aerospike.updateByKey”, “id”), where “id” is the name of the column that you’d \n",
    "#like to be the Primary key, while loading data from the DB.  \n",
    "\n",
    "insertDFWithSchema=spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".schema(schema) \\\n",
    ".option(\"aerospike.set\", \"salary_data\") \\\n",
    ".option(\"aerospike.updateByKey\", \"id\") \\\n",
    ".load()\n",
    "\n",
    "sqlView=\"inserttable\"\n",
    "\n",
    "\n",
    "#\n",
    "# V2 datasource doesn't allow insert into a view. \n",
    "#\n",
    "insertDFWithSchema.createTempView(sqlView)\n",
    "spark.sql(\"select * from inserttable\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame without specifying any Schema (uses schema inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame by using the Connector Schema inference mechanism\n",
    "# The fields preceded with __ are metadata fields - key/digest/expiry/generation/ttl\n",
    "# By default you just get everything, with no column ordering, which is why it looks untidy\n",
    "# Note we don't get anything in the 'key' field as we have not chosen to save as a bin.\n",
    "# Use .option(\"aerospike.sendKey\", True) to do this\n",
    "\n",
    "loadedDFWithoutSchema = (\n",
    "    spark.read.format(\"aerospike\") \\\n",
    "    .option(\"aerospike.set\", \"salary_data\") \\\n",
    "    .load()\n",
    ")\n",
    "\n",
    "loadedDFWithoutSchema.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame using user specified schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we explicitly set the schema, using the previously created schema object\n",
    "# we effectively type the rows in the Data Frame\n",
    "\n",
    "loadedDFWithSchema=spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".schema(schema) \\\n",
    ".option(\"aerospike.set\", \"salary_data\").load()\n",
    "\n",
    "loadedDFWithSchema.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Aerospike DB\n",
    "\n",
    "- Sample specified number of records from Aerospike to considerably reduce data movement between Aerospike and the Spark clusters. Depending on the aerospike.partition.factor setting, you may get more records than desired. Please use this property in conjunction with Spark `limit()` function to get the specified number of records. The sample read is not randomized, so sample more than you need and use the Spark `sample()` function to randomize if you see fit. You can use it in conjunction with `aerospike.recordspersecond` to control the load on the Aerospike server while sampling.\n",
    "\n",
    "- For more information, please see [documentation](https://docs.aerospike.com/docs/connect/processing/spark/configuration.html) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_spark_partitions (num_sp)=2^{aerospike.partition.factor}\n",
    "#total number of records = Math.ceil((float)aerospike.sample.size/num_sp) * (num_sp) \n",
    "#use lower partition factor for more accurate sampling\n",
    "setname=\"py_input_data\"\n",
    "sample_size=101\n",
    "\n",
    "df3=spark.read.format(\"aerospike\") \\\n",
    ".option(\"aerospike.partition.factor\",\"2\") \\\n",
    ".option(\"aerospike.set\",setname) \\\n",
    ".option(\"aerospike.sample.size\",\"101\") \\\n",
    ".load()\n",
    "\n",
    "df4=spark.read.format(\"aerospike\") \\\n",
    ".option(\"aerospike.partition.factor\",\"6\") \\\n",
    ".option(\"aerospike.set\",setname) \\\n",
    ".option(\"aerospike.sample.size\",\"101\") \\\n",
    ".load()\n",
    "\n",
    "#Notice that more records were read than requested due to the underlying partitioning logic related to the partition factor as described earlier, hence we use Spark limit() function additionally to return the desired number of records.\n",
    "count3=df3.count()\n",
    "count4=df4.count()\n",
    "\n",
    "\n",
    "#Note how limit got only 101 records from df4.\n",
    "dfWithLimit=df4.limit(101)\n",
    "limitCount=dfWithLimit.count()\n",
    "\n",
    "print(\"count3= \", count3, \" count4= \", count4, \" limitCount=\", limitCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Collection Data Types (CDT) in Aerospike\n",
    "\n",
    "### Save JSON into Aerospike using a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema specification\n",
    "aliases_type = StructType([\n",
    "    StructField(\"first_name\",StringType(),False),\n",
    "    StructField(\"last_name\",StringType(),False)\n",
    "])\n",
    "\n",
    "id_type = StructType([\n",
    "    StructField(\"first_name\",StringType(),False), \n",
    "    StructField(\"last_name\",StringType(),False), \n",
    "    StructField(\"aliases\",ArrayType(aliases_type),False)\n",
    "])\n",
    "\n",
    "street_adress_type = StructType([\n",
    "    StructField(\"street_name\",StringType(),False), \n",
    "    StructField(\"apt_number\",IntegerType(),False)\n",
    "])\n",
    "\n",
    "address_type = StructType([\n",
    "    StructField(\"zip\",LongType(),False), \n",
    "    StructField(\"street\",street_adress_type,False), \n",
    "    StructField(\"city\",StringType(),False)\n",
    "])\n",
    "\n",
    "workHistory_type = StructType([\n",
    "    StructField (\"company_name\",StringType(),False),\n",
    "    StructField( \"company_address\",address_type,False),\n",
    "    StructField(\"worked_from\",StringType(),False)\n",
    "])\n",
    "\n",
    "person_type = StructType([\n",
    "    StructField(\"name\",id_type,False),\n",
    "    StructField(\"SSN\",StringType(),False),\n",
    "    StructField(\"home_address\",ArrayType(address_type),False),\n",
    "    StructField(\"work_history\",ArrayType(workHistory_type),False)\n",
    "])\n",
    "\n",
    "# JSON data location\n",
    "complex_data_json=\"resources/nested_data.json\"\n",
    "\n",
    "# Read data in using prepared schema\n",
    "cmplx_data_with_schema=spark.read.schema(person_type).json(complex_data_json)\n",
    "\n",
    "# Save data to Aerospike\n",
    "cmplx_data_with_schema \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".format(\"aerospike\")  \\\n",
    ".option(\"aerospike.writeset\", \"complex_input_data\") \\\n",
    ".option(\"aerospike.updateByKey\", \"SSN\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve CDT from Aerospike into a DataFrame using schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedComplexDFWithSchema=spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".option(\"aerospike.set\", \"complex_input_data\") \\\n",
    ".schema(person_type) \\\n",
    ".load() \n",
    "loadedComplexDFWithSchema.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration with Aerospike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#convert Spark df to pandas df\n",
    "pdf = loadedDFWithSchema.toPandas()\n",
    "\n",
    "# Describe the data\n",
    "\n",
    "pdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram - Age\n",
    "age_min, age_max = int(np.amin(pdf['age'])), math.ceil(np.amax(pdf['age']))\n",
    "age_bucket_size = 5\n",
    "print(age_min,age_max)\n",
    "pdf[['age']].plot(kind='hist',bins=range(age_min,age_max,age_bucket_size),rwidth=0.8)\n",
    "plt.xlabel('Age',fontsize=10)\n",
    "plt.legend(loc=None)\n",
    "plt.show()\n",
    "\n",
    "#Histogram - Salary\n",
    "salary_min, salary_max = int(np.amin(pdf['salary'])), math.ceil(np.amax(pdf['salary']))\n",
    "salary_bucket_size = 5000\n",
    "pdf[['salary']].plot(kind='hist',bins=range(salary_min,salary_max,salary_bucket_size),rwidth=0.8)\n",
    "plt.xlabel('Salary',fontsize=10)\n",
    "plt.legend(loc=None)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap\n",
    "age_bucket_count = math.ceil((age_max - age_min)/age_bucket_size)\n",
    "salary_bucket_count = math.ceil((salary_max - salary_min)/salary_bucket_size)\n",
    "\n",
    "x = [[0 for i in range(salary_bucket_count)] for j in range(age_bucket_count)]\n",
    "for i in range(len(pdf['age'])):\n",
    "    age_bucket = math.floor((pdf['age'][i] - age_min)/age_bucket_size)\n",
    "    salary_bucket = math.floor((pdf['salary'][i] - salary_min)/salary_bucket_size)\n",
    "    x[age_bucket][salary_bucket] += 1\n",
    "\n",
    "plt.title(\"Salary/Age distribution heatmap\")\n",
    "plt.xlabel(\"Salary in '000s\")\n",
    "plt.ylabel(\"Age\")\n",
    "\n",
    "plt.imshow(x, cmap='YlOrRd', interpolation='nearest',extent=[salary_min/1000,salary_max/1000,age_min,age_max],\n",
    "           origin=\"lower\")\n",
    "plt.colorbar(orientation=\"horizontal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Quering Aerospike Data using SparkSQL\n",
    "\n",
    "### Note:\n",
    "   1. Queries that involve Primary Key or Digest in the predicate trigger aerospike_batch_get()( https://www.aerospike.com/docs/client/c/usage/kvs/batch.html) and run extremely fast. For e.g. a query containing `__key` or `__digest` with, with no `OR` between two bins.\n",
    "   2. All other queries may entail a full scan of the Aerospike DB if they can’t be converted to Aerospike batchget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that include Primary Key in the Predicate\n",
    "\n",
    "With batch get queries we can apply filters on metadata columns such as `__gen` or `__ttl`. To do this, these columns should be exposed through the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic PKey query\n",
    "batchGet1= spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".option(\"aerospike.set\", \"salary_data\") \\\n",
    ".option(\"aerospike.keyType\", \"int\") \\\n",
    ".load().where(\"__key = 100\") \\\n",
    "\n",
    "batchGet1.show()\n",
    "#Note ASDB only supports equality test with PKs in primary key query. \n",
    "#So, a where clause with \"__key >10\", would result in scan query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch get, primary key based query\n",
    "from pyspark.sql.functions import col\n",
    "somePrimaryKeys= list(range(1,10))\n",
    "someMoreKeys= list(range(12,14))\n",
    "batchGet2= spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".option(\"aerospike.set\", \"salary_data\") \\\n",
    ".option(\"aerospike.keyType\", \"int\") \\\n",
    ".load().where((col(\"__key\").isin(somePrimaryKeys)) | ( col(\"__key\").isin(someMoreKeys))) \n",
    "\n",
    "batchGet2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batchget query using `__digest`\n",
    "   - `__digest` can have only two types `BinaryType`(default type) or `StringType`.\n",
    "   - If schema is not provided and `__digest` is `StringType`, then set `aerospike.digestType` to `string`.\n",
    "   - Records retrieved with `__digest` batchget call will have null primary key (i.e.`__key` is `null`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert digests to a list of byte[]\n",
    "digest_list=batchGet2.select(\"__digest\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#convert digest to hex string for querying. Only digests of type hex string and byte[] array are allowed.\n",
    "string_digest=[ ''.join(format(x, '02x') for x in m) for m in digest_list]\n",
    "\n",
    "#option(\"aerospike.digestType\", \"string\") hints to assume that __digest type is string in schema inference.\n",
    "#please note that __key retrieved in this case is null. So be careful to use retrieved keys in downstream query!\n",
    "batchGetWithDigest= spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".option(\"aerospike.set\", \"salary_data\") \\\n",
    ".option(\"aerospike.digestType\", \"string\") \\\n",
    ".load().where(col(\"__digest\").isin(string_digest)) \n",
    "batchGetWithDigest.show()  \n",
    "\n",
    "\n",
    "#digests can be mixed with primary keys as well\n",
    "batchGetWithDigestAndKey= spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".option(\"aerospike.set\", \"salary_data\") \\\n",
    ".option(\"aerospike.digestType\", \"string\") \\\n",
    ".option(\"aerospike.keyType\", \"int\") \\\n",
    ".load().where(col(\"__digest\").isin(string_digest[0:1]) | ( col(\"__key\").isin(someMoreKeys))) \n",
    "batchGetWithDigestAndKey.show()\n",
    "#please note to the null in key columns in both dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries including non-primary key conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will run as a scan, which will be slower\n",
    "somePrimaryKeys= list(range(1,10))\n",
    "scanQuery1= spark \\\n",
    ".read \\\n",
    ".format(\"aerospike\") \\\n",
    ".option(\"aerospike.set\", \"salary_data\") \\\n",
    ".option(\"aerospike.keyType\", \"int\") \\\n",
    ".load().where((col(\"__key\").isin(somePrimaryKeys)) | ( col(\"age\") >50 ))\n",
    "\n",
    "scanQuery1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushdown [Aerospike Expressions](https://docs.aerospike.com/docs/guide/expressions/) from within a Spark API.\n",
    "\n",
    " - Make sure that you do not use no the WHERE clause or spark filters while querying\n",
    " - See [Aerospike Expressions](https://docs.aerospike.com/docs/guide/expressions/) for more information on how to construct expressions.\n",
    " - Contstructed expressions must be converted to Base64 before using them in the Spark API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_predexp= sc._jvm.com.aerospike.spark.utility.AerospikePushdownExpressions\n",
    "\n",
    "#id % 5 == 0  => get rows where mod(col(\"id\")) ==0\n",
    "#Equvalent java Exp: Exp.eq(Exp.mod(Exp.intBin(\"a\"), Exp.`val`(5)), Exp.`val`(0))\n",
    "expIntBin=scala_predexp.intBin(\"id\") # id is the name of column\n",
    "expMODIntBinEqualToZero=scala_predexp.eq(scala_predexp.mod(expIntBin, scala_predexp.val(5)),scala_predexp.val(0))\n",
    "expMODIntBinToBase64= scala_predexp.build(expMODIntBinEqualToZero).getBase64()\n",
    "#expMODIntBinToBase64= \"kwGTGpNRAqJpZAUA\"\n",
    "pushdownset = \"py_input_data\"\n",
    "\n",
    "\n",
    "pushDownDF =spark\\\n",
    "        .read \\\n",
    "        .format(\"aerospike\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"aerospike.set\", pushdownset) \\\n",
    "        .option(\"aerospike.pushdown.expressions\", expMODIntBinToBase64) \\\n",
    "        .load()\n",
    "\n",
    "pushDownDF.count() #should get 39 records, we have 199/5 records whose id bin is divisble by 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushDownDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for tuning Aerospike / Spark performance\n",
    "\n",
    "  - aerospike.partition.factor: number of logical aerospike partitions [0-15]\n",
    "  - aerospike.maxthreadcount : maximum number of threads to use for writing data into Aerospike\n",
    "  - aerospike.compression : compression of java client-server communication\n",
    "  - aerospike.batchMax : maximum number of records per read request (default 5000)\n",
    "  - aerospike.recordspersecond : same as java client\n",
    "\n",
    "## Other useful parameters\n",
    "  - aerospike.keyType : Primary key type hint for schema inference. Always set it properly if primary key type is not string  \n",
    "\n",
    "See https://www.aerospike.com/docs/connect/processing/spark/reference.html for detailed description of the above properties\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning using Aerospike / Spark\n",
    "\n",
    "In this section we use the data we took from Aerospike and apply a clustering algorithm to it.\n",
    "\n",
    "We assume the data is composed of multiple data sets having a Gaussian multi-variate distribution\n",
    "\n",
    "We don't know how many clusters there are, so we try clustering based on the assumption there are 1 through 20.\n",
    "\n",
    "We compare the quality of the results using the Bayesian Information Criterion - https://en.wikipedia.org/wiki/Bayesian_information_criterion and pick the best.\n",
    "    \n",
    "## Find Optimal Cluster Count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# We take the data we previously \n",
    "ages=pdf['age']\n",
    "salaries=pdf['salary']\n",
    "#age_salary_matrix=np.matrix([ages,salaries]).T\n",
    "age_salary_matrix=np.asarray([ages,salaries]).T\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "optimal_cluster_count = 1\n",
    "best_bic_score = GaussianMixture(1).fit(age_salary_matrix).bic(age_salary_matrix)\n",
    "\n",
    "for count in range(1,20):\n",
    "    gm=GaussianMixture(count)\n",
    "    gm.fit(age_salary_matrix)\n",
    "    if gm.bic(age_salary_matrix) < best_bic_score:\n",
    "        best_bic_score = gm.bic(age_salary_matrix)\n",
    "        optimal_cluster_count = count\n",
    "\n",
    "print(\"Optimal cluster count found to be \"+str(optimal_cluster_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate cluster distribution parameters\n",
    "Next we fit our cluster using the optimal cluster count, and print out the discovered means and covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(optimal_cluster_count)\n",
    "gm.fit(age_salary_matrix)\n",
    "\n",
    "estimates = []\n",
    "# Index\n",
    "for index in range(0,optimal_cluster_count):\n",
    "    estimated_mean_age = round(gm.means_[index][0],2)\n",
    "    estimated_mean_salary = round(gm.means_[index][1],0)\n",
    "    estimated_age_std_dev = round(math.sqrt(gm.covariances_[index][0][0]),2)\n",
    "    estimated_salary_std_dev = round(math.sqrt(gm.covariances_[index][1][1]),0)\n",
    "    estimated_correlation = round(gm.covariances_[index][0][1] / ( estimated_age_std_dev * estimated_salary_std_dev ),3)\n",
    "    row = [estimated_mean_age,estimated_mean_salary,estimated_age_std_dev,estimated_salary_std_dev,estimated_correlation]\n",
    "    estimates.append(row)\n",
    "    \n",
    "pd.DataFrame(estimates,columns = [\"Est Mean Age\",\"Est Mean Salary\",\"Est Age Std Dev\",\"Est Salary Std Dev\",\"Est Correlation\"])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Distribution Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_data_as_rows = []\n",
    "for distribution in distribution_data:\n",
    "    row = [distribution['age_mean'],distribution['salary_mean'],distribution['age_std_dev'],\n",
    "                             distribution['salary_std_dev'],distribution['age_salary_correlation']]\n",
    "    distribution_data_as_rows.append(row)\n",
    "\n",
    "pd.DataFrame(distribution_data_as_rows,columns = [\"Mean Age\",\"Mean Salary\",\"Age Std Dev\",\"Salary Std Dev\",\"Correlation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the algorithm provides good estimates of the original parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We generate new age/salary pairs for each of the distributions and look at how accurate the prediction is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(model,age_salary_distribution,sample_size):\n",
    "    # Generate new values\n",
    "    new_ages,new_salaries = age_salary_sample(age_salary_distribution,sample_size)\n",
    "    #new_age_salary_matrix=np.matrix([new_ages,new_salaries]).T\n",
    "    new_age_salary_matrix=np.asarray([new_ages,new_salaries]).T\n",
    "    # Find which cluster the mean would be classified into\n",
    "    #mean = np.matrix([age_salary_distribution['age_mean'],age_salary_distribution['salary_mean']])\n",
    "    #mean = np.asarray([age_salary_distribution['age_mean'],age_salary_distribution['salary_mean']])\n",
    "    mean = np.asarray(np.matrix([age_salary_distribution['age_mean'],age_salary_distribution['salary_mean']]))\n",
    "    mean_cluster_index = model.predict(mean)[0]\n",
    "    # How would new samples be classified\n",
    "    classification = model.predict(new_age_salary_matrix)\n",
    "    # How many were classified correctly\n",
    "    correctly_classified = len([ 1 for x in classification if x  == mean_cluster_index])\n",
    "    return correctly_classified / sample_size\n",
    "\n",
    "prediction_accuracy_results = [None for x in range(3)]\n",
    "for index, age_salary_distribution in enumerate(distribution_data):\n",
    "    prediction_accuracy_results[index] = prediction_accuracy(gm,age_salary_distribution,1000)\n",
    "\n",
    "overall_accuracy = sum(prediction_accuracy_results)/ len(prediction_accuracy_results)\n",
    "print(\"Accuracies for each distribution : \",\" ,\".join(map('{:.2%}'.format,prediction_accuracy_results)))\n",
    "print(\"Overall accuracy : \",'{:.2%}'.format(overall_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aerolookup\n",
    " aerolookup allows you to look up records corresponding to a set of keys stored in a Spark DF, streaming or otherwise. It supports:\n",
    " \n",
    "  - [Aerospike CDT](https://docs.aerospike.com/docs/guide/cdt.htmlarbitrary)\n",
    "  - Quota and retry (these configurations are extracted from sparkconf) \n",
    "  - [Flexible schema](https://docs.aerospike.com/docs/connect/processing/spark/configuration.html#flexible-schemas). To enable, set `aerospike.schema.flexible` to true in the SparkConf object.\n",
    "  - Aerospike Expressions Pushdown (Note: This must be specified through SparkConf object.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = StructType([StructField(\"first_name\", StringType(), False),\n",
    "                    StructField(\"last_name\", StringType(), False)])\n",
    "name = StructType([StructField(\"first_name\", StringType(), False),\n",
    "                   StructField(\"aliases\", ArrayType(alias), False)])\n",
    "street_adress = StructType([StructField(\"street_name\", StringType(), False),\n",
    "                            StructField(\"apt_number\", IntegerType(), False)])\n",
    "address = StructType([StructField(\"zip\", LongType(), False),\n",
    "                      StructField(\"street\", street_adress, False),\n",
    "                      StructField(\"city\", StringType(), False)])\n",
    "work_history = StructType([StructField(\"company_name\", StringType(), False),\n",
    "                          StructField(\"company_address\", address, False),\n",
    "                          StructField(\"worked_from\", StringType(), False)])\n",
    "\n",
    "output_schema = StructType([StructField(\"name\", name, False),\n",
    "                          StructField(\"SSN\", StringType(), False),\n",
    "                         StructField(\"home_address\", ArrayType(address), False)])\n",
    "\n",
    "ssns = [[\"825-55-3247\"], [\"289-18-1554\"], [\"756-46-4088\"], \n",
    "        [\"525-31-0299\"], [\"456-45-2200\"], [\"200-71-7765\"]]\n",
    "\n",
    "#Create a set of PKs whose records you'd like to look up in the Aerospike database\n",
    "customerIdsDF=spark.createDataFrame(ssns,[\"SSN\"])\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "scala_py_util= sc._jvm.com.aerospike.spark.PythonUtil #Import the scala object\n",
    "gateway_df=scala_py_util.aerolookup(\n",
    "    customerIdsDF._jdf,  #Please note ._jdf\n",
    "    'SSN',\n",
    "    'complex_input_data', #complex_input_data is the set in Aerospike database that you are using to look up the keys stored in SSN DF\n",
    "    output_schema.json(),\n",
    "    'test', \n",
    "    {} # may use this map to pass any aerospike configuration\n",
    ")\n",
    "aerolookup_df=pyspark.sql.DataFrame(gateway_df,spark._wrapped)  \n",
    "#Note the wrapping of java object into python.sql.DataFrame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aerolookup_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary index\n",
    "  - Secondary index query can be disabled by setting `aerospike.sindex.enable` to false (by default it is set to true).\n",
    "  - User can specify secondary index by setting `aerospike.sindex`. If it is not set, connector appropriately selects secondary index for query execution.\n",
    "  - User can also specify filter to use by setting `aerospike.sindex.filter`. This feature may be user to filter out CDT at the database site itself, which is not immediately possible to acheive using standard spark filters.\n",
    "  - Refer to the documentation for detailed discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create data for secondary index query demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_records=50\n",
    "si_schema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                        StructField(\"name\", StringType(), True),\n",
    "                       StructField(\"array\", ArrayType(IntegerType()), True)])\n",
    "si_set= \"py_si_set\"\n",
    "siBuf = []\n",
    "for i in range(1, si_records) :\n",
    "    id_ = i \n",
    "    name = \"name\"  + str(i)\n",
    "    arr= [x for x in range(i, i+3) ]\n",
    "    siBuf.append((id_, name, arr))    \n",
    "\n",
    "siRDD = spark.sparkContext.parallelize(siBuf)\n",
    "siInputDF=spark.createDataFrame(siRDD,si_schema)\n",
    "\n",
    "#Write the secondary index Data to Aerospike\n",
    "siInputDF.write.mode('overwrite').format(\"aerospike\")  \\\n",
    ".option(\"aerospike.writeset\", si_set).option(\"aerospike.updateByKey\", \"id\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  create and list secondary indices\n",
    "   - create secondary index `py_id_idx`, `py_name_idx` and `py_arr_idx` over respective bins.\n",
    "   - list the reated indices using connector `sindexList(namespace)` API. This API assumes that sparksession is alread created and contains informations such as hostname, namespace in spark runtime configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aerospike\n",
    "\n",
    "#index names to be created\n",
    "num_idx= \"py_id_idx\"\n",
    "str_idx= \"py_name_idx\"\n",
    "arr_idx= \"py_arr_idx\"\n",
    "\n",
    "import aerospike \n",
    "pyClient = aerospike.client({\"hosts\": [AS_HOST]}).connect()\n",
    "try:\n",
    "    pyClient.index_integer_create('test', si_set, \"id\", num_idx)\n",
    "    pyClient.index_string_create('test', si_set, \"name\", str_idx)\n",
    "    pyClient.index_list_create('test', si_set, 'array', aerospike.INDEX_NUMERIC, arr_idx)\n",
    "except ex.IndexFoundError as e:\n",
    "    print(e)\n",
    "\n",
    "#Print all \"test\" namsespace indices\n",
    "print(scala_py_util.sindexList(\"test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Index selection\n",
    "   - will be done automatically, if index is present in DB and `aerospike.sindex` is not set.\n",
    "   - user may set `aerospike.sindex` to indicate to use the set index for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically an appropriate secindary index is selected\n",
    "siIdDF = spark.read.format(\"aerospike\").schema(si_schema)\\\n",
    ".option(\"aerospike.set\", si_set)\\\n",
    ".option(\"aerospike.partition.factor\",1)\\\n",
    ".option(\"aerospike.sindex.enable\",\"true\")\\\n",
    ".load()\n",
    "siIdDF.where(siIdDF.id >= 40).show() #should get 10 records\n",
    "#search for `using secondary index: py_id_idx` in info logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user may set `aerospike.sindex` to use it for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user specified index \"aerospike.sindex\"\n",
    "siNameDF = spark.read.format(\"aerospike\").schema(si_schema)\\\n",
    ".option(\"aerospike.set\", si_set)\\\n",
    ".option(\"aerospike.partition.factor\",1)\\\n",
    ".option(\"aerospike.sindex\",\"py_name_idx\")\\\n",
    ".load()\n",
    "siNameDF.where(siNameDF.name == \"name1\").show() #should get 1 records\n",
    "#search for `using secondary index: py_name_idx` in info logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondary index query over CDT using user specified filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user specified filter in JSON format\n",
    "arrayQuery =r'''{ \"name\": \"array\", \"type\": \"NUMERIC\", \"colType\": 1, \"value\": 10 }''' # \"name\" is bin name, colType =1 indicates sindex over array datatype.\n",
    "siArrayDF = spark.read.format(\"aerospike\").schema(si_schema)\\\n",
    ".option(\"aerospike.set\", si_set).option(\"aerospike.partition.factor\",1)\\\n",
    ".option(\"aerospike.sindex.filter\",arrayQuery)\\\n",
    ".option(\"aerospike.sindex\",\"py_arr_idx\").load()\n",
    "siArrayDF.show() #should print 3 records, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
