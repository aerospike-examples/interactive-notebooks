{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model-Serving-with-Aerospike-Feature-Store\" data-toc-modified-id=\"Model-Serving-with-Aerospike-Feature-Store-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model Serving with Aerospike Feature Store</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Prerequisites\" data-toc-modified-id=\"Prerequisites-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prerequisites</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ensure-Database-Is-Running\" data-toc-modified-id=\"Ensure-Database-Is-Running-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Ensure Database Is Running</a></span></li><li><span><a href=\"#Initialize-Client\" data-toc-modified-id=\"Initialize-Client-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Initialize Client</a></span></li><li><span><a href=\"#Initialize-Spark\" data-toc-modified-id=\"Initialize-Spark-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Initialize Spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialize-Paths-and-Env-Variables\" data-toc-modified-id=\"Initialize-Paths-and-Env-Variables-1.3.3.1\"><span class=\"toc-item-num\">1.3.3.1&nbsp;&nbsp;</span>Initialize Paths and Env Variables</a></span></li><li><span><a href=\"#Configure-Spark-Session\" data-toc-modified-id=\"Configure-Spark-Session-1.3.3.2\"><span class=\"toc-item-num\">1.3.3.2&nbsp;&nbsp;</span>Configure Spark Session</a></span></li></ul></li><li><span><a href=\"#Access-Shell-Commands\" data-toc-modified-id=\"Access-Shell-Commands-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>Access Shell Commands</a></span></li></ul></li></ul></li><li><span><a href=\"#Prior-Context\" data-toc-modified-id=\"Prior-Context-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prior Context</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Data\" data-toc-modified-id=\"Feature-Data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Feature Data</a></span></li><li><span><a href=\"#Trained-Model\" data-toc-modified-id=\"Trained-Model-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Trained Model</a></span></li></ul></li><li><span><a href=\"#Components-of-Web-Service\" data-toc-modified-id=\"Components-of-Web-Service-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Components of Web Service</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-Model\" data-toc-modified-id=\"Loading-Model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Loading Model</a></span></li><li><span><a href=\"#Retrieving-Features\" data-toc-modified-id=\"Retrieving-Features-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Retrieving Features</a></span></li><li><span><a href=\"#Running-Model\" data-toc-modified-id=\"Running-Model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Running Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Construct-Feature-Vector\" data-toc-modified-id=\"Construct-Feature-Vector-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Construct Feature Vector</a></span></li><li><span><a href=\"#Predict\" data-toc-modified-id=\"Predict-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Predict</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-Serving-with-Web-Service\" data-toc-modified-id=\"Model-Serving-with-Web-Service-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Model Serving with Web Service</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Install-Web-Service-Framework\" data-toc-modified-id=\"Install-Web-Service-Framework-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Install Web Service Framework</a></span></li><li><span><a href=\"#Examine-Web-Service-File\" data-toc-modified-id=\"Examine-Web-Service-File-4.0.2\"><span class=\"toc-item-num\">4.0.2&nbsp;&nbsp;</span>Examine Web Service File</a></span></li><li><span><a href=\"#Run-Web-Service\" data-toc-modified-id=\"Run-Web-Service-4.0.3\"><span class=\"toc-item-num\">4.0.3&nbsp;&nbsp;</span>Run Web Service</a></span></li><li><span><a href=\"#Send-Requests-to-Web-Service\" data-toc-modified-id=\"Send-Requests-to-Web-Service-4.0.4\"><span class=\"toc-item-num\">4.0.4&nbsp;&nbsp;</span>Send Requests to Web Service</a></span></li></ul></li></ul></li><li><span><a href=\"#Takeaways-and-Conclusion\" data-toc-modified-id=\"Takeaways-and-Conclusion-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Takeaways and Conclusion</a></span></li><li><span><a href=\"#Cleaning-Up\" data-toc-modified-id=\"Cleaning-Up-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Cleaning Up</a></span></li><li><span><a href=\"#Further-Exploration-and-Resources\" data-toc-modified-id=\"Further-Exploration-and-Resources-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Further Exploration and Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Resources</a></span></li><li><span><a href=\"#Exploring-Other-Notebooks\" data-toc-modified-id=\"Exploring-Other-Notebooks-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Exploring Other Notebooks</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with Aerospike Feature Store\n",
    "This notebook is the third in the series of notebooks that show how Aerospike can be used as a feature store.\n",
    "\n",
    "This notebook requires the Aerospike Database and Spark running locally with Aerospike Spark Connector. To create a Docker container that satisfies the requirements and holds a copy of Aerospike notebooks, visit the [Aerospike Notebooks Repo](https://github.com/aerospike-examples/interactive-notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook shows how Aerospike can be used as a Feature Store for Machine Learning applications on Spark using Aerospike Spark Connector. It is Part 3 of the Feature Store series of notebooks, and focuses on Model Serving aspects concerning a Feature Store. The first two notebooks in the series discuss [Feature Engineering](feature-store-feature-eng.ipynb) and [Model Training](feature-store-model-training.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Reference Architecture](resources/fs-arch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is organized as follows:\n",
    "- Summary of the prior (Data Engineering and Model Training) notebooks\n",
    "- Load the trained and saved model for making a prediction. \n",
    "- Use Aerospike API to retrieve precomputed features. \n",
    "- Implement and test a web service that combines the above elements, that is, accesses features, runs the model and returns the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This tutorial assumes familiarity with the following topics:\n",
    "\n",
    "- [Aerospike Notebooks - Readme and Tips](../readme_tips.ipynb)\n",
    "- [Hello World](../python/hello_world.ipynb)\n",
    "- [Feature Store with Aerospike (Part 1)](feature-store-feature-eng.ipynb)\n",
    "- [Model Training with Aerospike Feature Store (Part 2)](feature-store-model-training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Set up Aerospike Server. Spark Server, and Spark Connector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure Database Is Running\n",
    "This notebook requires that Aerospike datbase is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aerospike database is running!\r\n"
     ]
    }
   ],
   "source": [
    "!asd >& /dev/null\n",
    "!pgrep -x asd >/dev/null && echo \"Aerospike database is running!\" || echo \"**Aerospike database is not running!**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Client\n",
    "Initialize Python Client used to access features stored in the Aerospike feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized and connected to database\n"
     ]
    }
   ],
   "source": [
    "import aerospike\n",
    "import sys\n",
    "# connect to the database\n",
    "config = {\n",
    "  'hosts': [ ('127.0.0.1', 3000) ]\n",
    "}\n",
    "try:\n",
    "  client = aerospike.client(config).connect()\n",
    "except:\n",
    "  print(\"failed to connect to the cluster with\", config['hosts'])\n",
    "  sys.exit(1)\n",
    "print('Client initialized and connected to database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark\n",
    "We will be using Spark functionality in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Paths and Env Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where spark notebook requisites are installed\n",
    "SPARK_NB_DIR = '/opt/spark-nb'\n",
    "SPARK_DIR = 'spark-dir-link'\n",
    "SPARK_HOME = SPARK_NB_DIR + '/' + SPARK_DIR\n",
    "AEROSPIKE_JAR = 'aerospike-jar-link'\n",
    "AEROSPIKE_JAR_PATH = SPARK_NB_DIR + '/' + AEROSPIKE_JAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP Address or DNS name for one host in your Aerospike cluster\n",
    "AS_HOST =\"localhost\"\n",
    "# Name of one of your namespaces. Type 'show namespaces' at the aql prompt if you are not sure\n",
    "AS_NAMESPACE = \"test\" \n",
    "AS_PORT = 3000 # Usually 3000, but change here if not\n",
    "AS_CONNECTION_STRING = AS_HOST + \":\"+ str(AS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the Spark installation using the SPARK_HOME parameter.\n",
    "import findspark\n",
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Aerospike Spark Connector jar in the command used to interact with Aerospike.\n",
    "import os \n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars ' + AEROSPIKE_JAR_PATH + ' pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Spark Session\n",
    "Please visit [Configuring Aerospike Connect for Spark](https://docs.aerospike.com/docs/connect/processing/spark/configuration.html) for more information about the properties used on this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, IntegerType, MapType, LongType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "conf=sc._conf.setAll([(\"aerospike.namespace\",AS_NAMESPACE),(\"aerospike.seedhost\",AS_CONNECTION_STRING)])\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Shell Commands\n",
    "You may execute shell commands including Aerospike tools like [aql](https://docs.aerospike.com/docs/tools/aql/index.html) and [asadm](https://docs.aerospike.com/docs/tools/asadm/index.html) in the terminal tab throughout this tutorial. Open a terminal tab by selecting File->Open from the notebook menu, and then New->Terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior Context\n",
    "In the prior two notebooks on Feature Enginnering and Model Training, we saved feature data to the feature store, and trained an ML model on it, respectively. If the saved feature data or the model is not available in this environment, you can run the following cells to recreate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Data\n",
    "Run the following cells ONLY IF the database does not have the feature data for credit card transactions from the prior notebooks (Part 1 or Part 2). You will need to covert them to `Code` cells before you can run them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# read and transform the sample credit card transactions data from a csv file\n",
    "from pyspark.sql.functions import expr\n",
    "df = spark.read.options(header=\"True\", inferSchema=\"True\") \\\n",
    "    .csv(\"resources/creditcard_small.csv\") \\\n",
    "    . orderBy(['_c0'], ascending=[True])\n",
    "new_col_names = ['CC1_' + (c if c != '_c0' else 'OldIdx') for c in df.columns]\n",
    "df = df.toDF(*new_col_names) \\\n",
    "        .withColumn('TxnId', expr('CC1_OldIdx+1').cast(StringType())) \\\n",
    "        .select(['TxnId','CC1_Class','CC1_Amount']+['CC1_V'+str(i) for i in range(1,29)])\n",
    "#df.toPandas().head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save feature values in entity records\n",
    "ENTITY_TYPE = 'cctxn'\n",
    "ID_COLUMN = 'TxnId'\n",
    "df.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .format(\"aerospike\")  \\\n",
    "    .option(\"aerospike.writeset\", ENTITY_TYPE+'-features')\\\n",
    "    .option(\"aerospike.updateByKey\", ID_COLUMN) \\\n",
    "    .save()\n",
    "\n",
    "print('Features stored to Feature Store.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model\n",
    "If the saved model `fs_model_rf` is not in the resources directory, first extract it from the archive using the following commands in the terminal tab:\n",
    "```\n",
    "cd /home/jovyan/notebooks/spark/resources\n",
    "tar -xvzf fs_model_rf.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of Web Service\n",
    "The main components of the web service are:\n",
    "1. Loading the model\n",
    "2. Retrieving the required feeatures\n",
    "3. Running the model to make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model\n",
    "Load the RandomForestClassifer model that we saved in the Model Training notebook (Part 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Random Forest Classification model.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "rf_model = RandomForestClassificationModel.read().load(\"resources/fs_model_rf\")\n",
    "print(\"Loaded Random Forest Classification model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Features\n",
    "The Python Client provides a convenient API to access specific features from the entity set as shown below. Recall, the model uses features CC1_V1 through CC1-V28. We also need to construct a schema for the dataframe which is needed to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CC1_V1</th>\n",
       "      <td>-1.158233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V2</th>\n",
       "      <td>0.877737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V3</th>\n",
       "      <td>1.548718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V4</th>\n",
       "      <td>0.403034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V5</th>\n",
       "      <td>-0.407193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V6</th>\n",
       "      <td>0.095921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V7</th>\n",
       "      <td>0.592941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V8</th>\n",
       "      <td>-0.270533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V9</th>\n",
       "      <td>0.817739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V10</th>\n",
       "      <td>0.753074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V11</th>\n",
       "      <td>-0.822843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V12</th>\n",
       "      <td>0.538196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V13</th>\n",
       "      <td>1.345852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V14</th>\n",
       "      <td>-1.119670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V15</th>\n",
       "      <td>0.175121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V16</th>\n",
       "      <td>-0.451449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V17</th>\n",
       "      <td>-0.237033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V18</th>\n",
       "      <td>-0.038195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V19</th>\n",
       "      <td>0.803487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V20</th>\n",
       "      <td>0.408542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V21</th>\n",
       "      <td>-0.009431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V22</th>\n",
       "      <td>0.798278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V23</th>\n",
       "      <td>-0.137458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V24</th>\n",
       "      <td>0.141267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V25</th>\n",
       "      <td>-0.206010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V26</th>\n",
       "      <td>0.502292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V27</th>\n",
       "      <td>0.219422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V28</th>\n",
       "      <td>0.215153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "CC1_V1  -1.158233\n",
       "CC1_V2   0.877737\n",
       "CC1_V3   1.548718\n",
       "CC1_V4   0.403034\n",
       "CC1_V5  -0.407193\n",
       "CC1_V6   0.095921\n",
       "CC1_V7   0.592941\n",
       "CC1_V8  -0.270533\n",
       "CC1_V9   0.817739\n",
       "CC1_V10  0.753074\n",
       "CC1_V11 -0.822843\n",
       "CC1_V12  0.538196\n",
       "CC1_V13  1.345852\n",
       "CC1_V14 -1.119670\n",
       "CC1_V15  0.175121\n",
       "CC1_V16 -0.451449\n",
       "CC1_V17 -0.237033\n",
       "CC1_V18 -0.038195\n",
       "CC1_V19  0.803487\n",
       "CC1_V20  0.408542\n",
       "CC1_V21 -0.009431\n",
       "CC1_V22  0.798278\n",
       "CC1_V23 -0.137458\n",
       "CC1_V24  0.141267\n",
       "CC1_V25 -0.206010\n",
       "CC1_V26  0.502292\n",
       "CC1_V27  0.219422\n",
       "CC1_V28  0.215153"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namespace = 'test'\n",
    "entity_set = 'cctxn-features'\n",
    "txnid = '5' # dummy value, the web service will get the id from the request params\n",
    "\n",
    "record_key = (namespace, entity_set, txnid)\n",
    "features = [\"CC1_V\"+str(i) for i in range(1,29)] # need features CC1_V1-CC1_V28\n",
    "schema = StructType()\n",
    "for i in range(1,29): # all features are of type float or Double\n",
    "    schema.add(\"CC1_V\"+str(i), DoubleType(), True)\n",
    "# get the needed features\n",
    "try:\n",
    "    (key, meta, bins) = client.select(record_key, features)\n",
    "except:\n",
    "  print('failed to get record')\n",
    "  sys.exit(1)\n",
    "\n",
    "# create an input dataframe for the model\n",
    "featureBuf = [tuple([bins[f] for f in features])]\n",
    "featureRDD = spark.sparkContext.parallelize(featureBuf)            \n",
    "featureDF = spark.createDataFrame(featureRDD, schema)\n",
    "featureDF.toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Feature Vector\n",
    "We first construct a feature vector from the input features as required by the model interface. The model only uses fvector column created by VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CC1_V1</th>\n",
       "      <td>-1.158233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V2</th>\n",
       "      <td>0.877737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V3</th>\n",
       "      <td>1.548718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V4</th>\n",
       "      <td>0.403034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V5</th>\n",
       "      <td>-0.407193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V6</th>\n",
       "      <td>0.095921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V7</th>\n",
       "      <td>0.592941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V8</th>\n",
       "      <td>-0.270533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V9</th>\n",
       "      <td>0.817739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V10</th>\n",
       "      <td>0.753074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V11</th>\n",
       "      <td>-0.822843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V12</th>\n",
       "      <td>0.538196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V13</th>\n",
       "      <td>1.345852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V14</th>\n",
       "      <td>-1.11967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V15</th>\n",
       "      <td>0.175121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V16</th>\n",
       "      <td>-0.451449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V17</th>\n",
       "      <td>-0.237033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V18</th>\n",
       "      <td>-0.038195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V19</th>\n",
       "      <td>0.803487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V20</th>\n",
       "      <td>0.408542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V21</th>\n",
       "      <td>-0.009431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V22</th>\n",
       "      <td>0.798278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V23</th>\n",
       "      <td>-0.137458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V24</th>\n",
       "      <td>0.141267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V25</th>\n",
       "      <td>-0.20601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V26</th>\n",
       "      <td>0.502292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V27</th>\n",
       "      <td>0.219422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC1_V28</th>\n",
       "      <td>0.215153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fvector</th>\n",
       "      <td>[-1.15823309349523, 0.877736754848451, 1.54871...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0\n",
       "CC1_V1                                           -1.158233\n",
       "CC1_V2                                            0.877737\n",
       "CC1_V3                                            1.548718\n",
       "CC1_V4                                            0.403034\n",
       "CC1_V5                                           -0.407193\n",
       "CC1_V6                                            0.095921\n",
       "CC1_V7                                            0.592941\n",
       "CC1_V8                                           -0.270533\n",
       "CC1_V9                                            0.817739\n",
       "CC1_V10                                           0.753074\n",
       "CC1_V11                                          -0.822843\n",
       "CC1_V12                                           0.538196\n",
       "CC1_V13                                           1.345852\n",
       "CC1_V14                                           -1.11967\n",
       "CC1_V15                                           0.175121\n",
       "CC1_V16                                          -0.451449\n",
       "CC1_V17                                          -0.237033\n",
       "CC1_V18                                          -0.038195\n",
       "CC1_V19                                           0.803487\n",
       "CC1_V20                                           0.408542\n",
       "CC1_V21                                          -0.009431\n",
       "CC1_V22                                           0.798278\n",
       "CC1_V23                                          -0.137458\n",
       "CC1_V24                                           0.141267\n",
       "CC1_V25                                           -0.20601\n",
       "CC1_V26                                           0.502292\n",
       "CC1_V27                                           0.219422\n",
       "CC1_V28                                           0.215153\n",
       "fvector  [-1.15823309349523, 0.877736754848451, 1.54871..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# create a feature vector from features\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"fvector\")\n",
    "featureVectorDF = assembler.transform(featureDF)\n",
    "featureVectorDF.toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict \n",
    "Call the model's transform function to predict. We input only a dataframe with `fvector` column, and use only two columns from the prediction datframe record: `probablity` and `prediction`. The threshold for fraud/no-fraud decision is 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal txn prob:  0.9505125242698634\n",
      "fraud prob:  0.049487475730136565\n",
      "prediction:  no fraud\n"
     ]
    }
   ],
   "source": [
    "rf_prediction = rf_model.transform(featureVectorDF['fvector',])\n",
    "result = rf_prediction['probability', 'prediction'].collect()[0]\n",
    "print('normal txn prob: ', result[0][0]) \n",
    "print('fraud prob: ', result[0][1])\n",
    "print('prediction: ', 'no fraud' if result[0][1] < 0.5  else 'fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving with Web Service\n",
    "Let's create a simple web service that serves the model. We will use the Flask framework to create the web service. The web service takes txnid as the query parameter, retrieves the features from the feature store, runs the model, and returns the prediction.\n",
    "\n",
    "Note, this model serving example is not realistic as we are using only precomputed features for inference. Also, we have trained and tested the model with the same data. Nonetheless, the example serves the purpose which is to illustrate the use of a feature store for model serving. It should not be difficult to use the patterns shown here to devise a realistic example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the existing spark session before starting the web service\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Web Service Framework\n",
    "Open a terminal tab, and install the Flask framework with the following command.\n",
    "\n",
    "```\n",
    "pip install flask\n",
    "pip install flask_restful\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Web Service File\n",
    "First open the file `resources/fs-model-ws.py` that implements the web service using Flask frameowrk, and examine its contents.\n",
    "\n",
    "Note that it is mostly the code in the above cells organized to run as a Flask web service. You can learn more about Flask [here](https://flask.palletsprojects.com/en/2.0.x/quickstart/#a-minimal-application)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Web Service\n",
    "Run the web service by opening a terminal tab and running the following commands in it:\n",
    "1. cd /home/jovyan/notebooks/spark/resources\n",
    "2. python fs-model-ws.py\n",
    "\n",
    "You can ignore the warning messages. After the \"Debugger is active\" message, the service is ready to receive requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Requests to Web Service\n",
    "Let's call the web service to predict the outcome for a transaction id. \n",
    "\n",
    "We can submit requests through the `curl` command as below. We can test with a few normal transaction (ids: 1, 2, 3, 10), and a few fraud transactions (ids: 6337, 120506, 150669). \n",
    "\n",
    "You can query the database to view other fraud and normal transaction ids. As you may recall, TxnId and CC1_Class are the bins for the transaction id and label respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"fraud_prob\": 0.052470997597310345, \r\n",
      "  \"normal_prob\": 0.9475290024026897, \r\n",
      "  \"prediction\": \"no fraud\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "# Send a request to the model web service running at 127.0.0.1:5000\n",
    "!curl http://127.0.0.1:5000/?txnid=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select TxnId, CC1_Class from test.cctxn-features where PK = '6337'\r\n",
      "+--------+------------+\r\n",
      "| TxnId  | CC1_Class  |\r\n",
      "+--------+------------+\r\n",
      "| \"6337\" | 1          |\r\n",
      "+--------+------------+\r\n",
      "1 row in set (0.000 secs)\r\n",
      "\r\n",
      "OK\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# You can query a transaction in the database. \n",
    "# remember CC1_Class is the label with 1 indicating a fraudulent transaction\n",
    "!aql -c \"select TxnId, CC1_Class from test.cctxn-features where PK = '6337'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways and Conclusion\n",
    "In this notebook, we explored how Aerospike can be used as a Feature Store for ML applications. Specifically, we showed how the precomputed features stored in the Aerospike feature store can be used at model serving time. We implemented a simple web service that loads the trained model, and then for each request, retrieves features, runs the model, and returns the model prediction. \n",
    "\n",
    "This is the third notebook in the series of notebooks on how Aerospike can be used as a feature store. The first and second notebooks discussed [Feature Engineering](feature-store-feature-eng.ipynb) and [Model Training](feature-store-model-training.ipynb) aspects respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up\n",
    "Shut down the web service by hitting Ctrl-C in the tab in which it is running.\n",
    "\n",
    "Close the spark session, and remove the tutorial data by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    ; # ignore\n",
    "# To remove all data in the namespace test, uncomment the following line and run: \n",
    "#!aql -c \"truncate test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exploration and Resources\n",
    "Here are some links for further exploration.\n",
    "\n",
    "## Resources\n",
    "- Related notebooks\n",
    "    - [Feature Store with Aerospike (Part 1)](feature-store-feature-eng.ipynb)\n",
    "    - [Model Training with Aerospike Feature Store (Part 2)](feature-store-model-training.ipynb)\n",
    "    - [Aerospike Connect for Spark Tutorial for Python](AerospikeSparkPython.ipynb)\n",
    "- Related blog posts\n",
    "    - [Let AI/ML workloads take off with Aerospike and Spark 3.0](https://medium.com/aerospike-developer-blog/let-ai-ml-workloads-take-off-with-aerospike-and-spark-3-0-82de2d834b99)\n",
    "    - [Using Aerospike Connect For Spark](https://medium.com/aerospike-developer-blog/aerospike-is-a-highly-scalable-key-value-database-offering-best-in-class-performance-5922450aaa78)\n",
    "- Aerospike Developer Hub\n",
    "    - [Developer Hub](https://developer.aerospike.com/)\n",
    "- Github repos\n",
    "    - [Spark Aerospike Example](https://github.com/aerospike-examples/spark-aerospike-example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Other Notebooks\n",
    "\n",
    "Visit [Aerospike notebooks repo](https://github.com/aerospike-examples/interactive-notebooks) to run additional Aerospike notebooks. To run a different notebook, download the notebook from the repo to your local machine, and then click on File->Open in the notebook menu, and select Upload."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
