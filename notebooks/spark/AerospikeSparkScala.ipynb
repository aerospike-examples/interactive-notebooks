{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerospike Spark Connector Tutorial for Scala\n",
    "\n",
    "## Tested with Spark connector 2.8.0, Java 8, Apache Spark 2.4.0, Python 3.7  and Scala 2.11.12 and Spylon ( https://pypi.org/project/spylon-kernel/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark \n",
    "launcher.jars = [\"aerospike-spark-assembly-2.8.0.jar\"] \n",
    "launcher.master = \"local[*]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.2:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1620415074373)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AS_HOST: String = 172.16.39.169:3000\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Specify the Seed Host of the Aerospike Server\n",
    "val AS_HOST = \"172.16.39.169:3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.ArrayBuffer\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.SaveMode\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SaveMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema in the Spark Connector\n",
    "\n",
    "-  Aerospike is schemaless, however spark adher to schema. After the schema is decided upon (either through inference or given), data within the bins must honor the types. \n",
    "\n",
    "- To infer the schema, the connector samples a set of records (configurable through `aerospike.schema.scan`) to decide the name of bins/columns and their types. This implies that the derived schema depends entirely upon sampled records.  \n",
    "\n",
    "- **Note that `__key` was not part of provided schema. So how can one query using `__key`? We can just add `__key` in provided schema with appropriate type. Similarly we can add `__gen` or `__ttl` etc.**  \n",
    "         \n",
    "      val schemaWithPK: StructType = new StructType(Array(\n",
    "                StructField(\"__key\",IntegerType, nullable = false),    \n",
    "                StructField(\"id\", IntegerType, nullable = false),\n",
    "                StructField(\"name\", StringType, nullable = false),\n",
    "                StructField(\"age\", IntegerType, nullable = false),\n",
    "                StructField(\"salary\",IntegerType, nullable = false)))\n",
    "                \n",
    "- **We recommend that you provide schema for queries that involve complex data types such as lists, maps, and mixed types. Using schema inference for CDT may cause unexpected issues.** \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexible schema inference \n",
    "\n",
    "Spark assumes that the underlying data store (Aerospike in this case) follows a strict schema for all the records within a table. However, Aerospike is a No-SQL DB and is schemaless. Hence a single bin (mapped to a column ) within a set ( mapped to a table ) could technically hold values of multiple Aerospike supported types. The Spark connector reconciles this incompatibility with help of certain rules. Please choose the configuration that suits your use case. The strict configuration (aerospike.schema.flexible = false ) could be used when you have modeled your data in Aerospike to adhere to a strict schema i.e. each record within the set has the same schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aerospike.schema.flexible = true (default) \n",
    "   \n",
    "  If none of the column types in the user-specified schema match the bin types of a record in Aerospike, a record with NULLs is returned in the result set. \n",
    "\n",
    "Please use the filter() in Spark to filter out NULL records. For e.g. df.filter(\"gender == NULL\").show(false), where df is a dataframe and gender is a field that was not specified in the user-specified schema. \n",
    "\n",
    "If the above mismatch is limited to fewer columns in the user-specified schema then NULL would be returned for those columns in the result set. **Note: there is no way to tell apart a NULL due to missing value in the original data set and the NULL due to mismatch, at this point. Hence, the user would have to treat all NULLs as missing values.** The columns that are not a part of the schema will be automatically filtered out in the result set by the connector.\n",
    "\n",
    "Please note that if any field is set to NOT nullable i.e. nullable = false, your query will error out if there’s a type mismatch between an Aerospike bin and the column type specified in the user-specified schema.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample data to demonstrate flexible schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|one| two|\n",
      "+---+----+\n",
      "| 82|  82|\n",
      "| 67|null|\n",
      "| 29|null|\n",
      "| 39|null|\n",
      "| 16|  16|\n",
      "| 34|  34|\n",
      "|  1|null|\n",
      "| 77|null|\n",
      "| 52|  52|\n",
      "| 27|null|\n",
      "| 25|null|\n",
      "| 11|null|\n",
      "| 15|null|\n",
      "| 96|  96|\n",
      "| 97|null|\n",
      "|  4|   4|\n",
      "| 89|null|\n",
      "| 14|  14|\n",
      "| 79|null|\n",
      "| 71|null|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import com.aerospike.client.policy.WritePolicy\n",
       "import com.aerospike.spark.sql.AerospikeConnection\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import com.aerospike.client.{AerospikeClient, AerospikeException, Bin, Key}\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@2d5fbbac\n",
       "client: com.aerospike.client.AerospikeClient = com.aerospike.client.AerospikeClient@36f19faf\n",
       "flexsetname: String = flexschema\n",
       "wp: com.aerospike.client.policy.WritePolicy = com.aerospike.client.policy.WritePolicy@6a18a2f1\n",
       "wp.expiration: Int = 6000\n",
       "spark2: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2876f127\n",
       "flexibleSchema: org.apache.spark.sql.types.StructType = StructType(StructField(one,IntegerType,true), StructField(two,IntegerType,true))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.aerospike.client.policy.WritePolicy\n",
    "import com.aerospike.spark.sql.AerospikeConnection\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import com.aerospike.client.{AerospikeClient, AerospikeException, Bin, Key}\n",
    "\n",
    "val conf = sc.getConf.clone();\n",
    "\n",
    "conf.set(\"aerospike.seedhost\" , AS_HOST)\n",
    "conf.set(\"aerospike.schema.flexible\" , \"true\") //by default it is always true\n",
    "\n",
    "val client = AerospikeConnection.getClient(conf)\n",
    "val flexsetname = \"flexschema\"\n",
    "val wp = new WritePolicy()\n",
    "    wp.expiration = 6000 // expire data in 10 minutes\n",
    "    for (i <- 1 to 100) {\n",
    "      val key = new Key(\"test\", flexsetname, i)\n",
    "      client.delete(null, key )\n",
    "      if( i %2 ==0){\n",
    "        client.put(wp, key, new Bin(\"one\", i.toInt), new Bin(\"two\", i.toInt))\n",
    "      }else{\n",
    "        client.put(wp, key, new Bin(\"one\", i.toInt), new Bin(\"two\", i.toString))\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "conf.set(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\")\n",
    "conf.set(\"aerospike.namespace\", \"test\")\n",
    "spark.close()\n",
    "\n",
    "val spark2= SparkSession.builder().config(conf).master(\"local[2]\").getOrCreate()\n",
    "val flexibleSchema= StructType (\n",
    "      Seq(\n",
    "        StructField(\"one\", IntegerType, true ),\n",
    "        StructField(\"two\", IntegerType, true )\n",
    "      )\n",
    "    )\n",
    "\n",
    "spark2.read.format(\"aerospike\").schema(flexibleSchema).option(\"aerospike.set\", flexsetname).load().show()\n",
    "\n",
    "//Please note that, in case of type mismatch all columns with odd value of `one`(which had string type) is set to null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aerospike.schema.flexible = false \n",
    "\n",
    "If a mismatch between the user-specified schema and the schema of a record in Aerospike is detected at the bin/column level, your query will error out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-07 12:18:01 ERROR Executor:91 - Exception in task 0.0 in stage 4.0 (TID 49)\n",
      "com.aerospike.spark.sql.TypeConverter$TypeMismatchException\n",
      "\tat com.aerospike.spark.sql.TypeConverter$.convertToSparkType(TypeConverter.scala:243)\n",
      "\tat com.aerospike.spark.sql.TypeConverter$.binToValue(TypeConverter.scala:314)\n",
      "\tat com.aerospike.spark.sql.RowIterator$$anonfun$23.apply(KeyRecordRDD.scala:638)\n",
      "\tat com.aerospike.spark.sql.RowIterator$$anonfun$23.apply(KeyRecordRDD.scala:627)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
      "\tat com.aerospike.spark.sql.RowIterator.get(KeyRecordRDD.scala:627)\n",
      "\tat com.aerospike.spark.sql.RowIterator.get(KeyRecordRDD.scala:607)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.next(DataSourceRDD.scala:59)\n",
      "\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2021-05-07 12:18:01 WARN  TaskSetManager:66 - Lost task 0.0 in stage 4.0 (TID 49, localhost, executor driver): com.aerospike.spark.sql.TypeConverter$TypeMismatchException\n",
      "\tat com.aerospike.spark.sql.TypeConverter$.convertToSparkType(TypeConverter.scala:243)\n",
      "\tat com.aerospike.spark.sql.TypeConverter$.binToValue(TypeConverter.scala:314)\n",
      "\tat com.aerospike.spark.sql.RowIterator$$anonfun$23.apply(KeyRecordRDD.scala:638)\n",
      "\tat com.aerospike.spark.sql.RowIterator$$anonfun$23.apply(KeyRecordRDD.scala:627)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n",
      "\tat com.aerospike.spark.sql.RowIterator.get(KeyRecordRDD.scala:627)\n",
      "\tat com.aerospike.spark.sql.RowIterator.get(KeyRecordRDD.scala:607)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.next(DataSourceRDD.scala:59)\n",
      "\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2021-05-07 12:18:01 ERROR TaskSetManager:70 - Task 0 in stage 4.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import scala.util.Try\n",
       "df: scala.util.Try[Unit] =\n",
       "Failure(org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 49, localhost, executor driver): com.aerospike.spark.sql.TypeConverter$TypeMismatchException\n",
       "\tat com.aerospike.spark.sql.TypeConverter$.convertToSparkType(TypeConverter.scala:243)\n",
       "\tat com.aerospike.spark.sql.TypeConverter$.binToValue(TypeConverter.scala:314)\n",
       "\tat com.aerospike.spark.sql.RowIterator$$anonfun$23.apply(KeyRecordRDD.scala:638)\n",
       "\tat com.aerospike.spark.sql.RowIterator$$anonfun$23.apply(KeyRecordRDD.scala:627)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sc..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//When strict matching is set, we will get an exception due to type mismatch with schema provided.\n",
    "import scala.util.Try\n",
    "\n",
    "val df = Try{\n",
    "    spark2.sqlContext.read.\n",
    "    format(\"aerospike\").\n",
    "    schema(flexibleSchema).\n",
    "    option(\"aerospike.schema.flexible\", \"false\").\n",
    "    option(\"aerospike.set\", flexsetname).\n",
    "    load().show()\n",
    "\n",
    "}            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample data and write it into Aerospike Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|  name|age|salary|\n",
      "+---+------+---+------+\n",
      "|  1| name1|  1| 63874|\n",
      "|  2| name2|  2| 80652|\n",
      "|  3| name3|  3| 89869|\n",
      "|  4| name4|  4| 90393|\n",
      "|  5| name5|  5| 54456|\n",
      "|  6| name6|  6| 97832|\n",
      "|  7| name7|  7| 56316|\n",
      "|  8| name8|  8| 87378|\n",
      "|  9| name9|  9| 99287|\n",
      "| 10|name10| 10| 80446|\n",
      "+---+------+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_records: Int = 1000\n",
       "rand: util.Random.type = scala.util.Random$@656ebfcb\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(name,StringType,false), StructField(age,IntegerType,false), StructField(salary,IntegerType,false))\n",
       "inputDF: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create test data\n",
    "\n",
    "val num_records=1000\n",
    "val rand = scala.util.Random\n",
    "\n",
    "//schema of input data\n",
    "// val spark = SparkSession.builder().config(strictConf).master(\"local[*]\").getOrCreate()\n",
    "val schema: StructType = new StructType(\n",
    "    Array(\n",
    "    StructField(\"id\", IntegerType, nullable = false),\n",
    "    StructField(\"name\", StringType, nullable = false),\n",
    "    StructField(\"age\", IntegerType, nullable = false),\n",
    "    StructField(\"salary\",IntegerType, nullable = false)\n",
    "  ))\n",
    "\n",
    "val inputDF = {\n",
    "    val inputBuf=  new ArrayBuffer[Row]()\n",
    "    for ( i <- 1 to num_records){\n",
    "        val name = \"name\"  + i\n",
    "        val age = i%100\n",
    "        val salary = 50000 + rand.nextInt(50000)\n",
    "        val id = i \n",
    "        val r = Row(id, name, age,salary)\n",
    "        inputBuf.append(r)\n",
    "    }\n",
    "    val inputRDD = spark2.sparkContext.parallelize(inputBuf.toSeq)\n",
    "    spark2.createDataFrame(inputRDD,schema)\n",
    "}\n",
    "\n",
    "inputDF.show(10)\n",
    "\n",
    "//Write the Sample Data to Aerospike\n",
    "inputDF.write.mode(SaveMode.Overwrite) \n",
    ".format(\"aerospike\") //aerospike specific format\n",
    ".option(\"aerospike.writeset\", \"scala_input_data\") //write to this set\n",
    ".option(\"aerospike.updateByKey\", \"id\") //indicates which columns should be used for construction of primary key\n",
    ".option(\"aerospike.sendKey\", \"true\")\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data using sql insert staements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|132|name132| 32| 96194|\n",
      "|647|name647| 47| 81584|\n",
      "| 45| name45| 45| 52189|\n",
      "|558|name558| 58| 89307|\n",
      "|608|name608|  8| 70123|\n",
      "|687|name687| 87| 57574|\n",
      "|372|name372| 72| 52430|\n",
      "|335|name335| 35| 70880|\n",
      "|911|name911| 11| 89581|\n",
      "|352|name352| 52| 77206|\n",
      "| 94| name94| 94| 58512|\n",
      "|890|name890| 90| 85952|\n",
      "|334|name334| 34| 59674|\n",
      "|907|name907|  7| 79228|\n",
      "|148|name148| 48| 97284|\n",
      "|315|name315| 15| 72954|\n",
      "|163|name163| 63| 77421|\n",
      "|882|name882| 82| 57553|\n",
      "|602|name602|  2| 76237|\n",
      "|673|name673| 73| 50336|\n",
      "+---+-------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "insertDFWithSchema: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n",
       "sqlView: String = inserttable\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "Aerospike DB needs a Primary key for record insertion. Hence, you must identify the primary key column \n",
    "using for example .option(“aerospike.updateByKey”, “id”), where “id” is the name of the column that you’d \n",
    "like to be the Primary key, while loading data from the DB.\n",
    "*/\n",
    "val insertDFWithSchema=spark2\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".schema(schema)\n",
    ".option(\"aerospike.updateByKey\", \"id\") //required for sql inserts \n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".load()\n",
    "\n",
    "val sqlView=\"inserttable\"\n",
    "insertDFWithSchema.createOrReplaceTempView(sqlView)\n",
    "//\n",
    "//V2 datasource doesn't allow insert into a view. \n",
    "//\n",
    "\n",
    "// spark.sql(s\"insert into $sqlView  values (20000, 'insert_record1', 200, 23000), (20001, 'insert_record2', 201, 23001)\")\n",
    "\n",
    "// spark\n",
    "// .sqlContext\n",
    "// .read\n",
    "// .format(\"aerospike\")\n",
    "// .schema(schema)\n",
    "// .option(\"aerospike.seedhost\",AS_HOST)\n",
    "// .option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    "// .option (\"aerospike.namespace\", \"test\")\n",
    "// .option(\"aerospike.set\", \"input_data\").load.where(\"id >2000\").show()\n",
    "spark2.sql(s\"select * from $sqlView\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame without specifying any schema i.e. using connector schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- __key: string (nullable = true)\n",
      " |-- __digest: binary (nullable = true)\n",
      " |-- __expiry: integer (nullable = false)\n",
      " |-- __generation: integer (nullable = false)\n",
      " |-- __ttl: integer (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedDFWithoutSchema: org.apache.spark.sql.DataFrame = [__key: string, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Spark DataFrame by using the Connector Schema inference mechanism\n",
    "\n",
    "val loadedDFWithoutSchema=spark2\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_input_data\") //read the data from this set\n",
    ".load\n",
    "loadedDFWithoutSchema.printSchema()\n",
    "//Notice that schema of loaded data has some additional fields. \n",
    "// When connector infers schema, it also adds internal metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame with user specified schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|132|name132| 32| 96194|\n",
      "|647|name647| 47| 81584|\n",
      "| 45| name45| 45| 52189|\n",
      "|558|name558| 58| 89307|\n",
      "|608|name608|  8| 70123|\n",
      "+---+-------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedDFWithSchema: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Data can be loaded with known schema as well.\n",
    "val loadedDFWithSchema=spark2\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".schema(schema)\n",
    ".option(\"aerospike.set\", \"scala_input_data\").load\n",
    "loadedDFWithSchema.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Sample Collection Data Types (CDT) data into Aerospike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- aliases: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |-- last_name: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- home_address: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = true)\n",
      " |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |-- work_history: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = true)\n",
      " |    |    |-- company_address: struct (nullable = true)\n",
      " |    |    |    |-- zip: long (nullable = true)\n",
      " |    |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- worked_from: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "complex_data_json: String = resources/nested_data.json\n",
       "alias: org.apache.spark.sql.types.StructType = StructType(StructField(first_name,StringType,false), StructField(last_name,StringType,false))\n",
       "name: org.apache.spark.sql.types.StructType = StructType(StructField(first_name,StringType,false), StructField(aliases,ArrayType(StructType(StructField(first_name,StringType,false), StructField(last_name,StringType,false)),true),false))\n",
       "street_adress: org.apache.spark.sql.types.StructType = StructType(StructField(street_name,StringType,false), StructField(apt_number,IntegerType,false))\n",
       "address: org.apache.spark.sql.types.StructType = StructType(StructField(zip,LongType,false), StructField(street,StructType(StructField(street_name,StringType,false), StructField(apt_number,IntegerType,false)),fal..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val complex_data_json=\"resources/nested_data.json\"\n",
    "val alias=  StructType(List(\n",
    "    StructField(\"first_name\",StringType, false),\n",
    "    StructField(\"last_name\",StringType, false)))\n",
    "\n",
    "  val name= StructType(List(\n",
    "    StructField(\"first_name\",StringType, false),\n",
    "    StructField(\"aliases\",ArrayType(alias), false )\n",
    "  ))\n",
    "\n",
    "  val street_adress= StructType(List(\n",
    "    StructField(\"street_name\", StringType, false),\n",
    "    StructField(\"apt_number\" , IntegerType, false)))\n",
    "\n",
    "  val address = StructType( List(\n",
    "    StructField (\"zip\" , LongType, false),\n",
    "    StructField(\"street\", street_adress, false),\n",
    "    StructField(\"city\", StringType, false)))\n",
    "\n",
    "  val workHistory = StructType(List(\n",
    "    StructField (\"company_name\" , StringType, false),\n",
    "    StructField( \"company_address\" , address, false),\n",
    "    StructField(\"worked_from\", StringType, false)))\n",
    "\n",
    "  val person=  StructType ( List(\n",
    "    StructField(\"name\" , name, false, Metadata.empty),\n",
    "    StructField(\"SSN\", StringType, false,Metadata.empty),\n",
    "    StructField(\"home_address\", ArrayType(address), false),\n",
    "    StructField(\"work_history\", ArrayType(workHistory), false)))\n",
    "\n",
    "val cmplx_data_with_schema=spark2.read.schema(person).json(complex_data_json)\n",
    "\n",
    "cmplx_data_with_schema.printSchema()\n",
    "cmplx_data_with_schema.write.mode(SaveMode.Overwrite) \n",
    ".format(\"aerospike\") //aerospike specific format\n",
    ".option(\"aerospike.seedhost\", AS_HOST) //db hostname, can be added multiple hosts, delimited with \":\"\n",
    ".option(\"aerospike.namespace\", \"test\") //use this namespace \n",
    ".option(\"aerospike.writeset\", \"scala_complex_input_data\") //write to this set\n",
    ".option(\"aerospike.updateByKey\", \"name.first_name\") //indicates which columns should be used for construction of primary key\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Complex Data Types (CDT) into a DataFrame with user specified schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- first_name: string (nullable = false)\n",
      " |    |-- aliases: array (nullable = false)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = false)\n",
      " |    |    |    |-- last_name: string (nullable = false)\n",
      " |-- SSN: string (nullable = false)\n",
      " |-- home_address: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = false)\n",
      " |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |-- city: string (nullable = false)\n",
      " |-- work_history: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = false)\n",
      " |    |    |-- company_address: struct (nullable = false)\n",
      " |    |    |    |-- zip: long (nullable = false)\n",
      " |    |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |    |-- city: string (nullable = false)\n",
      " |    |    |-- worked_from: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedComplexDFWithSchema: org.apache.spark.sql.DataFrame = [name: struct<first_name: string, aliases: array<struct<first_name:string,last_name:string>>>, SSN: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val loadedComplexDFWithSchema=spark2\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_complex_input_data\") //read the data from this set\n",
    ".schema(person)\n",
    ".load\n",
    "loadedComplexDFWithSchema.printSchema()\n",
    "//Please note the difference in types of loaded data in both cases. With schema, we extactly infer complex types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering Aerospike Data using SparkSQL\n",
    "\n",
    "### Things to keep in mind\n",
    "   1. Queries that involve Primary Key or Digest in the predicate trigger aerospike_batch_get()( https://www.aerospike.com/docs/client/c/usage/kvs/batch.html) and run extremely fast. For e.g. a query containing `__key` or `__digest` with, with no `OR` between two bins.\n",
    "   2. All other queries may entail a full scan of the Aerospike DB if they can’t be converted to Aerospike batchget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that include Primary Key in the Predicate\n",
    "\n",
    "In case of batchget queries we can also apply filters upon metadata columns like `__gen` or `__ttl` etc. To do so, these columns should be exposed through schema (if schema provided). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|   name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|  829|[C0 B6 C4 DE 68 D...|       0|           1|   -1|name829| 29| 79521|829|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batchGet1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchGet1= spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when schema is not provided.\n",
    ".load.where(\"__key = 829\")\n",
    "batchGet1.show()\n",
    "//Please be aware Aerospike database supports only equality test with PKs in primary key query. \n",
    "//So, a where clause with \"__key >10\", would result in scan query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+-----+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl| name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-----+---+------+---+\n",
      "|    1|[89 31 AB FE 54 D...|       0|           1|   -1|name1|  1| 63874|  1|\n",
      "|    4|[93 F1 65 F0 E8 9...|       0|           1|   -1|name4|  4| 90393|  4|\n",
      "|    3|[D4 A1 0B A5 12 0...|       0|           1|   -1|name3|  3| 89869|  3|\n",
      "|    7|[30 94 D4 E7 9E 8...|       0|           1|   -1|name7|  7| 56316|  7|\n",
      "|    5|[3E F5 94 A9 3A A...|       0|           1|   -1|name5|  5| 54456|  5|\n",
      "+-----+--------------------+--------+------------+-----+-----+---+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "somePrimaryKeys: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "someMoreKeys: scala.collection.immutable.Range = Range(12, 13, 14)\n",
       "batchGet2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//In this query we are doing *OR* between PK subqueries \n",
    "\n",
    "val somePrimaryKeys= 1.to(10).toSeq\n",
    "val someMoreKeys= 12.to(14).toSeq\n",
    "val batchGet2= spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when inferred without schema.\n",
    ".load.where((col(\"__key\") isin (somePrimaryKeys:_*)) || ( col(\"__key\") isin (someMoreKeys:_*) ))\n",
    "batchGet2.show(5)\n",
    "//We should got in total 13 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that do not include Primary Key in the Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|   name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|  558|[14 80 A2 9D D2 E...|       0|           1|   -1|name558| 58| 89307|558|\n",
      "|  687|[1A 30 21 88 39 A...|       0|           1|   -1|name687| 87| 57574|687|\n",
      "|  372|[1B 40 51 DD 64 F...|       0|           1|   -1|name372| 72| 52430|372|\n",
      "|  352|[23 A0 99 06 1F 7...|       0|           1|   -1|name352| 52| 77206|352|\n",
      "|   94|[26 E0 C4 85 CE 9...|       0|           1|   -1| name94| 94| 58512| 94|\n",
      "|  890|[26 30 F7 1A D3 A...|       0|           1|   -1|name890| 90| 85952|890|\n",
      "|  163|[3E D0 72 42 15 9...|       0|           1|   -1|name163| 63| 77421|163|\n",
      "|  882|[3E C0 28 CE F2 5...|       0|           1|   -1|name882| 82| 57553|882|\n",
      "|  673|[45 10 C1 D6 80 3...|       0|           1|   -1|name673| 73| 50336|673|\n",
      "|  991|[47 A0 D4 EC 12 1...|       0|           1|   -1|name991| 91| 78591|991|\n",
      "|  293|[48 40 20 B0 E6 D...|       0|           1|   -1|name293| 93| 92732|293|\n",
      "|  679|[57 80 24 4F 1D 3...|       0|           1|   -1|name679| 79| 95405|679|\n",
      "|  153|[5D E0 05 75 BF 3...|       0|           1|   -1|name153| 53| 62635|153|\n",
      "|  485|[6B 80 7E E1 A4 5...|       0|           1|   -1|name485| 85| 72672|485|\n",
      "|  997|[72 10 81 9D E2 E...|       0|           1|   -1|name997| 97| 76992|997|\n",
      "|  482|[85 B0 B1 3F 49 A...|       0|           1|   -1|name482| 82| 93910|482|\n",
      "|  166|[8A 00 3E 64 19 D...|       0|           1|   -1|name166| 66| 70551|166|\n",
      "|  590|[8C 20 A4 28 BE 7...|       0|           1|   -1|name590| 90| 93196|590|\n",
      "|  689|[9B 00 70 22 F0 8...|       0|           1|   -1|name689| 89| 98225|689|\n",
      "|  895|[9D A0 9D 91 AE 8...|       0|           1|   -1|name895| 95| 70134|895|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "somePrimaryKeys: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "scanQuery1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val somePrimaryKeys= 1.to(10).toSeq\n",
    "val scanQuery1= spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when inferred without schema.\n",
    ".load.where((col(\"__key\") isin (somePrimaryKeys:_*)) || ( col(\"age\") >50 ))\n",
    "\n",
    "scanQuery1.show()\n",
    "\n",
    "//Since there is OR between PKs and Bin. It will be treated as Scan query. \n",
    "//Primary keys are not stored in bins(by default), hence only filters corresponding to bins are honored.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batchget query using `__digest`\n",
    "   - `__digest` can have only two types `BinaryType`(default type) or `StringType`.\n",
    "   - If schema is not provided and `__digest` is `StringType`, then set `aerospike.digestType` to `string`.\n",
    "   - Records retrieved with `__digest` batchget call will have null primary key (i.e.`__key` is `null`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|   name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "| null|[C0 B6 C4 DE 68 D...|       0|           1|   -1|name829| 29| 79521|829|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|   name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "| null|c0b6c4de68d77d7b9...|       0|           1|   -1|name829| 29| 79521|829|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "\n",
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|            __digest|\n",
      "+---+-------+---+--------------------+\n",
      "|829|name829| 29|[C0 B6 C4 DE 68 D...|\n",
      "+---+-------+---+--------------------+\n",
      "\n",
      "+---+-------+---+--------------------+\n",
      "| id|   name|age|            __digest|\n",
      "+---+-------+---+--------------------+\n",
      "|829|name829| 29|c0b6c4de68d77d7b9...|\n",
      "+---+-------+---+--------------------+\n",
      "\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|   name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|  829|[C0 B6 C4 DE 68 D...|       0|           1|   -1|name829| 29| 79521|829|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import com.aerospike.client.Key\n",
       "import com.aerospike.spark.utility.HelperFunctions._\n",
       "v: Int = 829\n",
       "ByteArray2Hex: (bytes: Array[Byte])String\n",
       "binaryDigest: Array[Byte] = Array(-64, -74, -60, -34, 104, -41, 125, 123, -99, -102, 85, 105, 107, -58, 71, 15, -85, -124, -21, 98)\n",
       "stringDigest: String = c0b6c4de68d77d7b9d9a55696bc6470fab84eb62\n",
       "batchGetdigest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: string, __digest: binary ... 7 more fields]\n",
       "batchGetStringdigest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: string, __digest: string ... 7 more fields]\n",
       "binaryDigestSchema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(name,StringType,false), StructField(age,IntegerType,false), StructField(__digest,Binary..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.aerospike.client.Key\n",
    "import com.aerospike.spark.utility.HelperFunctions._\n",
    "val v: Int = 829\n",
    "\n",
    "def ByteArray2Hex(bytes: Array[Byte]): String = {\n",
    "    bytes.map(\"%02x\".format(_)).mkString\n",
    "}\n",
    "\n",
    "//using schema inference \n",
    "val binaryDigest= new Key(\"test\",\"scala_input_data\",v).digest\n",
    "val stringDigest= ByteArray2Hex(binaryDigest) //convert Array[Byte] digest to hexstring\n",
    "\n",
    "val batchGetdigest= spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".load.filter($\"__digest\"=== binaryDigest )\n",
    "batchGetdigest.show() //note __key is null in retrieved record\n",
    "\n",
    "val batchGetStringdigest= spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".option(\"aerospike.digestType\", \"string\") //specify digestType if using schema inference \n",
    ".load.filter($\"__digest\"=== stringDigest )\n",
    "batchGetStringdigest.show()\n",
    "\n",
    "//using user provided schema, with BinaryType digest\n",
    "//Note that the retrieved records __key field is null. \n",
    "val binaryDigestSchema: StructType = new StructType(\n",
    "    Array(\n",
    "    StructField(\"id\", IntegerType, nullable = false),\n",
    "    StructField(\"name\", StringType, nullable = false),\n",
    "    StructField(\"age\", IntegerType, nullable = false),\n",
    "    StructField(\"__digest\",BinaryType, nullable = false)  //note to query using digest, schema should have this field\n",
    "  ))\n",
    "val dfBinaryDigest=spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".schema(binaryDigestSchema)\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".load.filter($\"__digest\"=== binaryDigest )\n",
    "dfBinaryDigest.show()\n",
    "\n",
    "//using user provided schema, with hex string digest\n",
    "val stringDigestSchema: StructType = new StructType(\n",
    "    Array(\n",
    "    StructField(\"id\", IntegerType, nullable = false),\n",
    "    StructField(\"name\", StringType, nullable = false),\n",
    "    StructField(\"age\", IntegerType, nullable = false),\n",
    "    StructField(\"__digest\",StringType, nullable = false)  //note the type of digest here!\n",
    "  ))\n",
    "val dfStringDigest=spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".schema(stringDigestSchema)\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".load.filter($\"__digest\"=== stringDigest )\n",
    "dfStringDigest.show()\n",
    "\n",
    "\n",
    "//Please note that in scan call, if data was written using sendKey=true, then __key value will retrieved.\n",
    "val dfBinaryDigestWithScan=spark2.sqlContext\n",
    ".read\n",
    ".format(\"aerospike\")\n",
    ".option(\"aerospike.set\", \"scala_input_data\")\n",
    ".load.filter($\"__digest\"=== binaryDigest || $\"id\" === 829 ) // \n",
    "dfBinaryDigestWithScan.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query with CDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|                name|        SSN|        home_address|        work_history|           past_jobs|num_jobs|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|[Jamie, [[Patrici...|569-31-4715|[[53379, [James I...|[[Brown, Miller a...|[Brown, Miller an...|       5|\n",
      "|[Michael, [[Micha...|455-56-8642|[[2300, [Bauer Ov...|[[Harrington, All...|[Harrington, Alle...|       5|\n",
      "|[Luis, [[David, G...|818-16-1742|[[60659, [Oneill ...|[[Moss-Johnson, [...|[Moss-Johnson, St...|       5|\n",
      "|[Tami, [[Joseph, ...|001-49-0685|[[23288, [Clark V...|[[Roberts PLC, [4...|[Roberts PLC, Hub...|       5|\n",
      "|[Krista, [[Robert...|756-24-3462|[[64750, [Thomas ...|[[Baker PLC, [468...|[Baker PLC, Kirk ...|       5|\n",
      "|[Kristina, [[Vick...|545-62-3152|[[70288, [Rebecca...|[[Vaughn Inc, [20...|[Vaughn Inc, Brow...|       5|\n",
      "|[Elizabeth, [[And...|394-89-8545|[[45347, [Pierce ...|[[Gaines, Gray an...|[Gaines, Gray and...|       5|\n",
      "|[Bob, [[Theresa, ...|751-73-2267|[[25939, [Floyd H...|[[Cortez-Roberts,...|[Cortez-Roberts, ...|       5|\n",
      "|[Julie, [[Kyle, W...|845-58-5322|[[773, [Sarah Gre...|[[Kelly Group, [7...|[Kelly Group, Aus...|       5|\n",
      "|[Melissa, [[Erica...|002-25-1920|[[66571, [Derek C...|[[Bradford Ltd, [...|[Bradford Ltd, Gl...|       5|\n",
      "|[Sara, [[Ian, Bar...|749-22-5723|[[82017, [Coleman...|[[Anderson LLC, [...|[Anderson LLC, St...|       5|\n",
      "|[Christopher, [[J...|868-70-5021|[[13016, [Wood La...|[[Potter Inc, [60...|[Potter Inc, Pric...|       5|\n",
      "|[Samantha, [[Sele...|788-03-5996|[[60114, [Christo...|[[Haley, Barnett ...|[Haley, Barnett a...|       5|\n",
      "|[Christine, [[Gin...|411-04-6557|[[75021, [Shelia ...|[[Bass-Roth, [688...|[Bass-Roth, Torre...|       5|\n",
      "|[Fred, [[Andrew, ...|566-85-7824|[[91508, [Dawson ...|[[Harrell-Smith, ...|[Harrell-Smith, S...|       5|\n",
      "|[Jose, [[Ryan, Pu...|237-85-4119|[[51096, [Cassidy...|[[Armstrong and S...|[Armstrong and So...|       5|\n",
      "|[Laura, [[Mackenz...|145-75-4980|[[87038, [Isaiah ...|[[Williams, Thoma...|[Williams, Thomas...|       5|\n",
      "|[Beth, [[Julia, M...|745-90-4193|[[73086, [Thompso...|[[Allen, Smith an...|[Allen, Smith and...|       5|\n",
      "|[Robert, [[Jared,...|588-24-1668|[[28724, [Koch Is...|[[Anderson, Moon ...|[Anderson, Moon a...|       5|\n",
      "|[George, [[Stacy,...|368-19-0912|[[54976, [Angela ...|[[Smith-Petersen,...|[Smith-Petersen, ...|       5|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Find all people who have atleast 5 jobs in past.\n",
    "loadedComplexDFWithSchema\n",
    ".withColumn(\"past_jobs\", col(\"work_history.company_name\"))\n",
    ".withColumn(\"num_jobs\", size(col(\"past_jobs\")))\n",
    ".where(col(\"num_jobs\")  >4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from Aerospike DB\n",
    "\n",
    "   - interplay of `aerospike.partition.factor` and `aerospike.sample.size`\n",
    "   - `aerospike.partition.factor` : Decides how many spark partitions will be created (default 8, means 2^8 spark parititons, tune it based on the resource)\n",
    "   - `aerospike.sample.size` : Approximately fetch specifed number of records from DB (avoids complete DB scan, default 0 means fetch everything from DB).\n",
    "   - If you wish to get exact number then couple it with `limit`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "setname: String = scala_input_data\n",
       "sample_size: Int = 101\n",
       "df3: org.apache.spark.sql.DataFrame = [__key: string, __digest: binary ... 7 more fields]\n",
       "df4: org.apache.spark.sql.DataFrame = [__key: string, __digest: binary ... 7 more fields]\n",
       "count3: Long = 104\n",
       "count4: Long = 128\n",
       "dfWithLimit: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: string, __digest: binary ... 7 more fields]\n",
       "limitCount: Long = 101\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//number_of_spark_partitions (num_sp)=2^{aerospike.partition.factor}\n",
    "//total number of records = Math.ceil((float)aerospike.sample.size/num_sp) * (num_sp) \n",
    "//use lower partition factor for more accurate sampling\n",
    "val setname=\"scala_input_data\"\n",
    "val sample_size=101\n",
    "\n",
    "val df3=spark2.read.format(\"aerospike\")\n",
    ".option(\"aerospike.partition.factor\",\"2\")\n",
    ".option(\"aerospike.set\",setname)\n",
    ".option(\"aerospike.sample.size\",\"101\") //allows to sample approximately spacific number of record.  \n",
    ".load()\n",
    "\n",
    "val df4=spark2.read.format(\"aerospike\")\n",
    ".option(\"aerospike.partition.factor\",\"6\")\n",
    ".option(\"aerospike.set\",setname)\n",
    ".option(\"aerospike.sample.size\",\"101\") //allows to sample approximately spacific number of record.  \n",
    ".load()\n",
    "\n",
    "//Note since we were not able to divide evenly, we endup fetching almost 100 records\n",
    "//notice the variation.\n",
    "val count3=df3.count()\n",
    "val count4=df4.count()\n",
    "\n",
    "//Note how limit got only 101 record from df4 which have 128 records.\n",
    "val dfWithLimit=df4.limit(101)\n",
    "val limitCount=dfWithLimit.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Aerospike Spark Connector Configuration properties in the Spark API to improve performance\n",
    "\n",
    "aerospike.partition.factor: number of logical aerospike partitions [0-15]\n",
    "aerospike.maxthreadcount : maximum number of threads to use for writing data into Aerospike\n",
    "aerospike.compression : compression of java client-server communication\n",
    "aerospike.batchMax : maximum number of records per read request (default 5000)\n",
    "aerospike.recordspersecond : same as java client\n",
    "\n",
    "#### Other\n",
    "- aerospike.keyType : Primary key type hint for schema inference. Always set it properly if primary key type is not string\n",
    "- aerospike.digestType : Digest type hint for schema inference. Always set it properly if digest type is not byte[]\n",
    "\n",
    "See https://www.aerospike.com/docs/connect/processing/spark/reference.html for detailed description of the above properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
