{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerospike Connect for Spark Tutorial for Python\n",
    "## Tested with Java 8, Spark 2.4.0, and Python 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP Address or DNS name for one host in your Aerospike cluster\n",
    "AS_HOST =\"10.0.2.106\"\n",
    "# Name of one of your namespaces. Type 'show namespaces' at the aql prompt if you are not sure\n",
    "AS_NAMESPACE = \"test\" \n",
    "\n",
    "AS_PORT = 3000 # Usually 3000, but change here if not\n",
    "AS_CONNECTION_STRING = AS_HOST + \":\"+ str(AS_PORT)\n",
    "\n",
    "# Download the Aerospike Spark jar\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "AEROSPIKE_SPARK_JAR_VERSION=\"2.4.0\"\n",
    "\n",
    "def aerospike_spark_jar_download_url(version=AEROSPIKE_SPARK_JAR_VERSION):\n",
    "    DOWNLOAD_PREFIX=\"https://www.aerospike.com/enterprise/download/connectors/aerospike-spark/\"\n",
    "    DOWNLOAD_SUFFIX=\"/artifact/jar\"\n",
    "    AEROSPIKE_SPARK_JAR_DOWNLOAD_URL = DOWNLOAD_PREFIX+AEROSPIKE_SPARK_JAR_VERSION+DOWNLOAD_SUFFIX\n",
    "    return AEROSPIKE_SPARK_JAR_DOWNLOAD_URL\n",
    "\n",
    "def download_aerospike_spark_jar(version=AEROSPIKE_SPARK_JAR_VERSION):\n",
    "    JAR_NAME=\"aerospike-spark-assembly-\"+AEROSPIKE_SPARK_JAR_VERSION+\".jar\"\n",
    "    if(not(os.path.exists(JAR_NAME))) :\n",
    "        urllib.request.urlretrieve(aerospike_spark_jar_download_url(),JAR_NAME)\n",
    "    else :\n",
    "        print(JAR_NAME+\" already downloaded\")\n",
    "    return os.path.join(os.getcwd(),JAR_NAME)\n",
    "\n",
    "AEROSPIKE_JAR_PATH=download_aerospike_spark_jar()\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = '--jars ' + AEROSPIKE_JAR_PATH + ' pyspark-shell'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, IntegerType, MapType, LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up spark and point aerospike db to AS_HOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark.conf.set(\"aerospike.namespace\", AS_NAMESPACE)\n",
    "spark.conf.set(\"aerospike.seedhost\", AS_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample data and write to Aerospike database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_records=1000\n",
    "\n",
    "schema = StructType( \n",
    "    [\n",
    "        StructField(\"_id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"salary\",IntegerType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "inputBuf = []\n",
    "for  i in range(1, num_records) :\n",
    "         name = \"name\"  + str(i)\n",
    "         age = i%100\n",
    "         salary = 50000 + random.randint(1,50000)\n",
    "         id_ = i \n",
    "         inputBuf.append((id_, name, age,salary))\n",
    "    \n",
    "inputRDD = spark.sparkContext.parallelize(inputBuf)\n",
    "inputDF=spark.createDataFrame(inputRDD,schema)\n",
    "\n",
    "\n",
    "#Write the Sample Data to Aerospike\n",
    "inputDF \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".format(\"com.aerospike.spark.sql\")  \\\n",
    ".option(\"aerospike.namespace\", AS_NAMESPACE) \\\n",
    ".option(\"aerospike.set\", \"py_input_data\")\\\n",
    ".option(\"aerospike.updateByKey\", \"_id\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema in the Spark Connector\n",
    "\n",
    "-  Aerospike is schemaless, however spark adher to schema. After the schema is decided upon (either through inference or given), data within the bins must honor the types. \n",
    "\n",
    "- To infer schema, the connector samples a set of records (configurable through `aerospike.schema.scan`) to decide the name of bins/columns and their types. This implies that the derived schema depends entirely upon sampled records.  \n",
    "\n",
    "- Note that `__key` was not part of provided schema. So how can one query using `__key`? We can just add `__key` in provided schema with appropriate type. Similarly we can add `__gen` or `__ttl` etc.  \n",
    "         \n",
    "      schemaWithPK =  StructType([\n",
    "                StructField(\"__key\",IntegerType(), False),    \n",
    "                StructField(\"id\", IntegerType(), False),\n",
    "                StructField(\"name\", StringType(), False),\n",
    "                StructField(\"age\", IntegerType(), False),\n",
    "                StructField(\"salary\",IntegerType(), False)])\n",
    "                \n",
    "- We recommend that you provide schema for queries that involve complex data types such as lists, maps, and mixed types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame without specifying any Schema (using connector schema inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|age|   name|salary|_id|\n",
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "| null|[02 50 2D 45 89 D...|       0|           1|   -1|  4|name604| 61568|604|\n",
      "| null|[08 50 66 A1 68 1...|       0|           1|   -1| 10| name10| 88282| 10|\n",
      "| null|[0B 70 8A F2 9F A...|       0|           1|   -1| 86|name586| 94012|586|\n",
      "| null|[13 D0 09 FD 8E E...|       0|           1|   -1| 77|name477| 97299|477|\n",
      "| null|[13 50 C4 E1 3F 6...|       0|           1|   -1| 50| name50| 90621| 50|\n",
      "| null|[17 B0 1E 54 2C 9...|       0|           1|   -1| 54|name554| 54781|554|\n",
      "| null|[18 10 E6 C8 15 6...|       0|           1|   -1| 64|name964| 71373|964|\n",
      "| null|[1C 30 1B 8B DC E...|       0|           1|   -1| 80|name880| 54376|880|\n",
      "| null|[25 B0 10 82 C7 6...|       0|           1|   -1| 59|name859| 99939|859|\n",
      "| null|[26 90 85 67 25 2...|       0|           1|   -1| 27|name227| 75497|227|\n",
      "| null|[28 C0 4F 1C 24 9...|       0|           1|   -1| 12|name812| 94863|812|\n",
      "| null|[35 D0 BA A7 35 7...|       0|           1|   -1| 80|name780| 71442|780|\n",
      "| null|[3C 80 4A 91 AA 3...|       0|           1|   -1| 55|name555| 82947|555|\n",
      "| null|[3D C0 3E 1E 0D C...|       0|           1|   -1| 85|name185| 79060|185|\n",
      "| null|[3E C0 46 29 33 C...|       0|           1|   -1| 21|name221| 82627|221|\n",
      "| null|[3E 00 D1 C3 29 7...|       0|           1|   -1| 17|name117| 93491|117|\n",
      "| null|[3F 50 25 02 8F 9...|       0|           1|   -1|  6|name706| 69972|706|\n",
      "| null|[47 10 14 58 CA 7...|       0|           1|   -1| 58|name358| 78507|358|\n",
      "| null|[59 10 C6 C4 20 7...|       0|           1|   -1| 38|name438| 64639|438|\n",
      "| null|[5B 30 96 AE 0B B...|       0|           1|   -1| 72|name872| 81794|872|\n",
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame by using the Connector Schema inference mechanism\n",
    "\n",
    "loadedDFWithoutSchema = (\n",
    "    spark.read.format(\"com.aerospike.spark.sql\") \\\n",
    "    .option(\"aerospike.seedhost\",AS_HOST) \\\n",
    "    .option (\"aerospike.namespace\", AS_NAMESPACE) \\\n",
    "    .option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") \\\n",
    "    .option(\"aerospike.set\", \"py_input_data\") \\\n",
    "    .load()\n",
    ")\n",
    "\n",
    "\n",
    "loadedDFWithoutSchema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame with user specified schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "|_id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|604|name604|  4| 61568|\n",
      "| 10| name10| 10| 88282|\n",
      "|586|name586| 86| 94012|\n",
      "|477|name477| 77| 97299|\n",
      "| 50| name50| 50| 90621|\n",
      "+---+-------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data can be loaded with known schema as well.\n",
    "\n",
    "loadedDFWithSchema=spark \\\n",
    ".read \\\n",
    ".format(\"com.aerospike.spark.sql\") \\\n",
    ".schema(schema) \\\n",
    ".option(\"aerospike.seedhost\",AS_HOST) \\\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \\\n",
    ".option (\"aerospike.namespace\", AS_NAMESPACE) \\\n",
    ".option(\"aerospike.set\", \"py_input_data\").load()\n",
    "\n",
    "loadedDFWithSchema.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkML with Aerospike\n",
    "\n",
    "### The purpose of this ML model is to illustrate how data in Aerospike can be used for training and inference using SparkML \n",
    "#### A K-Means clustering model is used to create several segments based on age and salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+--------------------+\n",
      "|_id|   name|age|salary|            features|\n",
      "+---+-------+---+------+--------------------+\n",
      "|604|name604|  4| 61568| [604.0,4.0,61568.0]|\n",
      "| 10| name10| 10| 88282| [10.0,10.0,88282.0]|\n",
      "|586|name586| 86| 94012|[586.0,86.0,94012.0]|\n",
      "|477|name477| 77| 97299|[477.0,77.0,97299.0]|\n",
      "| 50| name50| 50| 90621| [50.0,50.0,90621.0]|\n",
      "|554|name554| 54| 54781|[554.0,54.0,54781.0]|\n",
      "|964|name964| 64| 71373|[964.0,64.0,71373.0]|\n",
      "|880|name880| 80| 54376|[880.0,80.0,54376.0]|\n",
      "|859|name859| 59| 99939|[859.0,59.0,99939.0]|\n",
      "|227|name227| 27| 75497|[227.0,27.0,75497.0]|\n",
      "|812|name812| 12| 94863|[812.0,12.0,94863.0]|\n",
      "|780|name780| 80| 71442|[780.0,80.0,71442.0]|\n",
      "|555|name555| 55| 82947|[555.0,55.0,82947.0]|\n",
      "|185|name185| 85| 79060|[185.0,85.0,79060.0]|\n",
      "|221|name221| 21| 82627|[221.0,21.0,82627.0]|\n",
      "|117|name117| 17| 93491|[117.0,17.0,93491.0]|\n",
      "|706|name706|  6| 69972| [706.0,6.0,69972.0]|\n",
      "|358|name358| 58| 78507|[358.0,58.0,78507.0]|\n",
      "|438|name438| 38| 64639|[438.0,38.0,64639.0]|\n",
      "|872|name872| 72| 81794|[872.0,72.0,81794.0]|\n",
      "+---+-------+---+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Silhouette with squared euclidean distance = 0.7687480082530719\n",
      "Cluster Centers: \n",
      "[5.03029817e+02 4.98188073e+01 6.25371261e+04]\n",
      "[4.97653641e+02 4.93410302e+01 8.69270426e+04]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "#All machine learning algorithms in Spark take as input a Vector type, which must be a set of numerical values.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"_id\", \"age\", \"salary\"],\n",
    "    outputCol=\"features\")\n",
    "data_2 = assembler.transform(loadedDFWithSchema)\n",
    "data_2.show()\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(data_2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(data_2)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Sample Complex Data Types (CDT) data into Aerospike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- aliases: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |-- last_name: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- home_address: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = true)\n",
      " |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |-- work_history: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = true)\n",
      " |    |    |-- company_address: struct (nullable = true)\n",
      " |    |    |    |-- zip: long (nullable = true)\n",
      " |    |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- worked_from: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_json=\"resources/nested_data.json\"\n",
    "alias=  StructType( [\n",
    "    StructField(\"first_name\",StringType(), False),\n",
    "    StructField(\"last_name\",StringType(), False)]\n",
    ")\n",
    "\n",
    "name= StructType([\n",
    "    StructField(\"first_name\",StringType(), False), \n",
    "    StructField(\"aliases\",ArrayType(alias), False)]\n",
    ")\n",
    "street_adress= StructType([StructField(\"street_name\", StringType(), False), StructField(\"apt_number\", IntegerType(), False)])\n",
    "address = StructType([StructField(\"zip\", LongType(), False), StructField(\"street\", street_adress, False), StructField(\"city\", StringType(), False)])\n",
    "\n",
    "workHistory = StructType([StructField (\"company_name\" , StringType(), False),\n",
    "                              StructField( \"company_address\" , address, False),\n",
    "                              StructField(\"worked_from\", StringType(), False)]\n",
    "                        )\n",
    "\n",
    "person= StructType([StructField(\"name\", name, False),\n",
    "                        StructField(\"SSN\", StringType(), False),\n",
    "                        StructField(\"home_address\", ArrayType(address), False),\n",
    "                        StructField(\"work_history\", ArrayType(workHistory), False)]\n",
    "                  )\n",
    "\n",
    "cmplx_data_with_schema=spark.read.schema(person).json(complex_data_json)\n",
    "cmplx_data_with_schema.printSchema()\n",
    "\n",
    "cmplx_data_with_schema \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".format(\"com.aerospike.spark.sql\")  \\\n",
    ".option(\"aerospike.seedhost\", AS_HOST) \\\n",
    ".option(\"aerospike.namespace\", AS_NAMESPACE) \\\n",
    ".option(\"aerospike.writeset\", \"complex_input_data\") \\\n",
    ".option(\"aerospike.updateByKey\", \"name.first_name\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Complex Data Types (CDT) into a DataFrame without specifying any schema (using connector schema inference)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- __key: string (nullable = true)\n",
      " |-- __digest: binary (nullable = false)\n",
      " |-- __expiry: integer (nullable = false)\n",
      " |-- __generation: integer (nullable = false)\n",
      " |-- __ttl: integer (nullable = false)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- work_history: array (nullable = true)\n",
      " |    |-- element: binary (containsNull = true)\n",
      " |-- name: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- home_address: array (nullable = true)\n",
      " |    |-- element: binary (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loadedComplexDFWithoutSchema=spark \\\n",
    ".read \\\n",
    ".format(\"com.aerospike.spark.sql\") \\\n",
    ".option(\"aerospike.seedhost\", AS_HOST) \\\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") \\\n",
    ".option (\"aerospike.namespace\", \"test\") \\\n",
    ".option(\"aerospike.set\", \"complex_input_data\") \\\n",
    ".load() \n",
    "loadedComplexDFWithoutSchema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Complex Data Types (CDT) into a DataFrame with user specified schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- first_name: string (nullable = false)\n",
      " |    |-- aliases: array (nullable = false)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = false)\n",
      " |    |    |    |-- last_name: string (nullable = false)\n",
      " |-- SSN: string (nullable = false)\n",
      " |-- home_address: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = false)\n",
      " |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |-- city: string (nullable = false)\n",
      " |-- work_history: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = false)\n",
      " |    |    |-- company_address: struct (nullable = false)\n",
      " |    |    |    |-- zip: long (nullable = false)\n",
      " |    |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |    |-- city: string (nullable = false)\n",
      " |    |    |-- worked_from: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loadedComplexDFWithSchema=spark \\\n",
    ".read \\\n",
    ".format(\"com.aerospike.spark.sql\") \\\n",
    ".option(\"aerospike.seedhost\", AS_HOST) \\\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") \\\n",
    ".option (\"aerospike.namespace\", \"test\") \\\n",
    ".option(\"aerospike.set\", \"complex_input_data\") \\\n",
    ".schema(person) \\\n",
    ".load() \n",
    "loadedComplexDFWithSchema.printSchema()\n",
    "#Please note the difference in types of loaded data in both cases. With schema, we extactly infer complex types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration with Aerospike "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.7/site-packages (1.19.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.2-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 17.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in ./venv/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 60.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2020.06.20\n",
      "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 83.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in ./venv/lib/python3.7/site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in ./venv/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Installing collected packages: cycler, kiwisolver, pillow, certifi, matplotlib\n",
      "Successfully installed certifi-2020.6.20 cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.2 pillow-7.2.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.2-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.5 MB 16.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in ./venv/lib/python3.7/site-packages (from pandas) (1.19.2)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 77.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in ./venv/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.2 pytz-2020.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install packages, if not installed.\n",
    "!pip install numpy \n",
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-33ccbd6248ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mInt8Dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringDtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from pandas.core.indexes.api import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mCategoricalIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/groupby/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGroupBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = [\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m from pandas.core.aggregation import (\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mmaybe_mangle_lambdas\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mreconstruct_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/aggregation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFrameOrSeriesUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCachedAccessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrays\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_docs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_shared_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_percentiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint_thing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedeltas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimedeltaIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprinting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjustify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpprint_thing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\"Common IO api utilities\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.localpython/lib/python3.7/bz2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthreading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_bz2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBZ2Compressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBZ2Decompressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_bz2'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Pandas >= 0.19.2 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6eafada9810c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#convert spark df to pandas df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadedDFWithSchema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(pdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2082\u001b[0m         \"\"\"\n\u001b[1;32m   2083\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_minimum_pandas_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m         \u001b[0mrequire_minimum_pandas_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.7-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhave_pandas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         raise ImportError(\"Pandas >= %s must be installed; however, \"\n\u001b[0;32m--> 129\u001b[0;31m                           \"it was not found.\" % minimum_pandas_version)\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimum_pandas_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         raise ImportError(\"Pandas >= %s must be installed; however, \"\n",
      "\u001b[0;31mImportError\u001b[0m: Pandas >= 0.19.2 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "#convert spark df to pandas df\n",
    "pdf = loadedDFWithSchema.toPandas()\n",
    "#print(pdf)\n",
    "\n",
    "#Histogram\n",
    "pdf[['age']].head(20).plot(kind='hist',bins=[0,20,40,60,80,100],rwidth=0.8)\n",
    "plt.show()\n",
    "\n",
    "#Bar graph\n",
    "pdf.head(20).groupby('age')['name'].nunique().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "#pie chart\n",
    "pdf.head(20).groupby(['age']).sum().plot(kind='pie', y='salary')\n",
    "plt.show()\n",
    "\n",
    "#Line Plot\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "pdf.tail(20).plot(kind='line',x='name',y='salary', color='red', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Aerospike Data using SparkSQL\n",
    "#### Things to keep in mind\n",
    "   1. Queries that involve Primary Key in the predicate trigger aerospike_batch_get()[hyper link: https://www.aerospike.com/docs/client/c/usage/kvs/batch.html] and run extremely fast. For e.g. a query containing `__key` with, with no `OR` between two bins.\n",
    "   2. All other queries may entail a full scan of the Aerospike DB if they can’t be converted to Aerospike batchget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that include Primary Key in the Predicate\n",
    "\n",
    "In case of batchget queries we can also apply filters upon metadata columns like `__gen` or `__ttl` etc. To do so, these columns should be exposed through schema (if schema provided). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+------------+-------+---+-------+------+---+\n",
      "|__key|            __digest| __expiry|__generation|  __ttl|age|   name|salary|_id|\n",
      "+-----+--------------------+---------+------------+-------+---+-------+------+---+\n",
      "|  829|[9A E9 5B 0A 11 6...|340420991|          14|2591946| 29|name829| 55725|829|\n",
      "+-----+--------------------+---------+------------+-------+---+-------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batchGet1= spark \\\n",
    ".read \\\n",
    ".format(\"com.aerospike.spark.sql\") \\\n",
    ".option(\"aerospike.seedhost\", AS_HOST) \\\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \\\n",
    ".option (\"aerospike.namespace\", namespace) \\\n",
    ".option(\"aerospike.set\", \"py_input_data\") \\\n",
    ".option(\"aerospike.keyType\", \"int\") \\\n",
    ".load().where(\"__key = 829\") \\\n",
    "\n",
    "batchGet1.show()\n",
    "#Please be aware ASDB only supports equality test with PKs in primary key query. \n",
    "#So, a where clause with \"__key >10\", would result in scan query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+------------+-------+---+------+------+---+\n",
      "|__key|            __digest| __expiry|__generation|  __ttl|age|  name|salary|_id|\n",
      "+-----+--------------------+---------+------------+-------+---+------+------+---+\n",
      "|    4|[FE E0 77 E4 17 F...|340420991|          14|2591946|  4| name4| 87361|  4|\n",
      "|    5|[FF 00 39 4A 07 0...|340420991|          14|2591946|  5| name5| 96477|  5|\n",
      "|    7|[8B F3 60 83 F9 6...|340420991|          14|2591946|  7| name7| 87085|  7|\n",
      "|   13|[D7 B4 65 3D FA 4...|340420991|          14|2591946| 13|name13| 67846| 13|\n",
      "|    3|[75 25 0A 1D C0 4...|340420991|          14|2591946|  3| name3| 92078|  3|\n",
      "+-----+--------------------+---------+------------+-------+---+------+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In this query we are doing *OR* between PK subqueries \n",
    "from pyspark.sql.functions import *\n",
    "somePrimaryKeys= list(range(1,10))\n",
    "someMoreKeys= list(range(12,14))\n",
    "batchGet2= spark \\\n",
    ".read \\\n",
    ".format(\"com.aerospike.spark.sql\") \\\n",
    ".option(\"aerospike.seedhost\",AS_HOST) \\\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \\\n",
    ".option (\"aerospike.namespace\", namespace) \\\n",
    ".option(\"aerospike.set\", \"py_input_data\") \\\n",
    ".option(\"aerospike.keyType\", \"int\") \\\n",
    ".load().where((col(\"__key\").isin(somePrimaryKeys)) | ( col(\"__key\").isin(someMoreKeys))) \n",
    "\n",
    "batchGet2.show(5)\n",
    "#We should got in total 'len(somePrimaryKeys) + len(someMoreKeys)' records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that do not include Primary Key in the Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+------------+-------+---+-------+------+---+\n",
      "|__key|            __digest| __expiry|__generation|  __ttl|age|   name|salary|_id|\n",
      "+-----+--------------------+---------+------------+-------+---+-------+------+---+\n",
      "| null|[0B 70 8A F2 9F A...|340420991|          14|2591945| 86|name586| 55387|586|\n",
      "| null|[13 D0 09 FD 8E E...|340420991|          14|2591945| 77|name477| 87042|477|\n",
      "| null|[17 B0 1E 54 2C 9...|340420991|          14|2591945| 54|name554| 66669|554|\n",
      "| null|[18 10 E6 C8 15 6...|340420991|          14|2591945| 64|name964| 95080|964|\n",
      "| null|[1C 30 1B 8B DC E...|340420991|          14|2591945| 80|name880| 70626|880|\n",
      "| null|[25 B0 10 82 C7 6...|340420991|          14|2591945| 59|name859| 64936|859|\n",
      "| null|[35 D0 BA A7 35 7...|340420991|          14|2591945| 80|name780| 51597|780|\n",
      "| null|[3C 80 4A 91 AA 3...|340420991|          14|2591945| 55|name555| 72318|555|\n",
      "| null|[3D C0 3E 1E 0D C...|340420991|          14|2591945| 85|name185| 53643|185|\n",
      "| null|[47 10 14 58 CA 7...|340420991|          14|2591945| 58|name358| 81450|358|\n",
      "| null|[5B 30 96 AE 0B B...|340420991|          14|2591945| 72|name872| 53033|872|\n",
      "| null|[5C A0 3F 4E DA D...|340420991|          14|2591945| 61|name961| 97882|961|\n",
      "| null|[5E E0 15 0A CE 3...|340420991|          14|2591945| 88| name88| 82346| 88|\n",
      "| null|[5F B0 F2 C5 21 E...|340420991|          14|2591945| 72|name672| 70492|672|\n",
      "| null|[67 50 44 9E D8 0...|340420991|          14|2591945| 70|name970| 70938|970|\n",
      "| null|[78 10 D6 CA 04 8...|340420991|          14|2591945| 77| name77| 53461| 77|\n",
      "| null|[7C 50 EC 73 DF 5...|340420991|          14|2591945| 98|name498| 51740|498|\n",
      "| null|[7D 20 83 6F A9 E...|340420991|          14|2591945| 93|name893| 57191|893|\n",
      "| null|[8C 30 BA 1A 1C 1...|340420991|          14|2591945| 72|name272| 72588|272|\n",
      "| null|[9B 00 20 57 9B 2...|340420991|          14|2591945| 71|name971| 98439|971|\n",
      "+-----+--------------------+---------+------------+-------+---+-------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "somePrimaryKeys= list(range(1,10))\n",
    "scanQuery1= spark \\\n",
    ".read \\\n",
    ".format(\"com.aerospike.spark.sql\") \\\n",
    ".option(\"aerospike.seedhost\", AS_HOST) \\\n",
    ".option (\"aerospike.namespace\", namespace) \\\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \\\n",
    ".option(\"aerospike.set\", \"py_input_data\") \\\n",
    ".option(\"aerospike.keyType\", \"int\") \\\n",
    ".load().where((col(\"__key\").isin(somePrimaryKeys)) | ( col(\"age\") >50 ))\n",
    "\n",
    "scanQuery1.show()\n",
    "\n",
    "#Since there is OR between PKs and Bin. It will be treated as Scan query. \n",
    "#Primary keys are not stored in bins(by default), hence only filters corresponding to bins are honored.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query with CDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|                name|        SSN|        home_address|        work_history|           past_jobs|num_jobs|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|[Tami, [[Joseph, ...|001-49-0685|[[23288, [Clark V...|[[Roberts PLC, [4...|[Roberts PLC, Hub...|       5|\n",
      "|[Chelsea, [[Melis...|465-88-7213|[[49305, [Ward By...|[[Ochoa and Sons,...|[Ochoa and Sons, ...|       5|\n",
      "|[Jonathan, [[Robe...|526-54-7792|[[71421, [William...|[[Henderson-Shaw,...|[Henderson-Shaw, ...|       5|\n",
      "|[Gary, [[Cameron,...|825-55-3247|[[66428, [Kim Mil...|[[Bishop, Scott a...|[Bishop, Scott an...|       5|\n",
      "|[Danielle, [[Mich...|319-30-0983|[[63276, [Bauer C...|[[Powers LLC, [60...|[Powers LLC, Powe...|       5|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find people who have had at least 5 jobs in the past\n",
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "loadedComplexDFWithSchema \\\n",
    ".withColumn(\"past_jobs\", col(\"work_history.company_name\")) \\\n",
    ".withColumn(\"num_jobs\", size(col(\"past_jobs\")))  \\\n",
    ".where(col(\"num_jobs\") > 4) \\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Aerospike Spark Connector Configuration properties in the Spark API to improve performance \n",
    "\n",
    "  - aerospike.partition.factor: number of logical aerospike partitions [0-15]\n",
    "  - aerospike.maxthreadcount : maximum number of threads to use for writing data into Aerospike\n",
    "  - aerospike.compression : compression of java client-server communication\n",
    "  - aerospike.batchMax : maximum number of records per read request (default 5000)\n",
    "  - aerospike.recordspersecond : same as java client\n",
    "\n",
    "#### Other\n",
    "  - aerospike.keyType : Primary key type hint for schema inference. Always set it properly if primary key type is not string  \n",
    "\n",
    "See https://www.aerospike.com/docs/connect/processing/spark/reference.html for detailed description of the above properties\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
