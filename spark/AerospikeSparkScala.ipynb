{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerospike Spark Connector Tutorial for Scala\n",
    "\n",
    "## Tested with Java 8, Spark 2.4.0, Python 3.7,  Scala 2.11.12, and  Spylon ( https://pypi.org/project/spylon-kernel/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.jars = [\"aerospike-spark-assembly-2.4.0.jar\"] \n",
    "launcher.master = \"local[*]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1600130426123)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AS_HOST: String = 127.0.0.1:3000\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Specify the Seed Host of the Aerospike Server\n",
    "val AS_HOST =\"127.0.0.1:3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.ArrayBuffer\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.SaveMode\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SaveMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample data and write it into Aerospike Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|  name|age|salary|\n",
      "+---+------+---+------+\n",
      "|  1| name1|  1| 79452|\n",
      "|  2| name2|  2| 80775|\n",
      "|  3| name3|  3| 76844|\n",
      "|  4| name4|  4| 55969|\n",
      "|  5| name5|  5| 71474|\n",
      "|  6| name6|  6| 73131|\n",
      "|  7| name7|  7| 98801|\n",
      "|  8| name8|  8| 60590|\n",
      "|  9| name9|  9| 74402|\n",
      "| 10|name10| 10| 53572|\n",
      "+---+------+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_records: Int = 1000\n",
       "rand: util.Random.type = scala.util.Random$@e4e4235\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(name,StringType,false), StructField(age,IntegerType,false), StructField(salary,IntegerType,false))\n",
       "inputDF: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create test data\n",
    "\n",
    "val num_records=1000\n",
    "val rand = scala.util.Random\n",
    "\n",
    "//schema of input data\n",
    "\n",
    "val schema: StructType = new StructType(\n",
    "    Array(\n",
    "    StructField(\"id\", IntegerType, nullable = false),\n",
    "    StructField(\"name\", StringType, nullable = false),\n",
    "    StructField(\"age\", IntegerType, nullable = false),\n",
    "    StructField(\"salary\",IntegerType, nullable = false)\n",
    "  ))\n",
    "\n",
    "val inputDF = {\n",
    "    val inputBuf=  new ArrayBuffer[Row]()\n",
    "    for ( i <- 1 to num_records){\n",
    "        val name = \"name\"  + i\n",
    "        val age = i%100\n",
    "        val salary = 50000 + rand.nextInt(50000)\n",
    "        val id = i \n",
    "        val r = Row(id, name, age,salary)\n",
    "        inputBuf.append(r)\n",
    "    }\n",
    "    val inputRDD = spark.sparkContext.parallelize(inputBuf.toSeq)\n",
    "    spark.createDataFrame(inputRDD,schema)\n",
    "}\n",
    "\n",
    "inputDF.show(10)\n",
    "\n",
    "//Write the Sample Data to Aerospike\n",
    "inputDF.write.mode(SaveMode.Overwrite) \n",
    ".format(\"com.aerospike.spark.sql\") //aerospike specific format\n",
    ".option(\"aerospike.seedhost\", AS_HOST) //db hostname, can be added multiple hosts, delimited with \":\"\n",
    ".option(\"aerospike.namespace\", \"test\") //use this namespace \n",
    ".option(\"aerospike.writeset\", \"input_data\") //write to this set\n",
    ".option(\"aerospike.updateByKey\", \"id\") //indicates which columns should be used for construction of primary key\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema in the Spark Connector\n",
    "\n",
    "-  Aerospike is schemaless, however spark adher to schema. After schema is decided upon (either through inference or given), data within the bins must honor the types. \n",
    "\n",
    "- To infer the schema, the connector samples a set of records (configurable through `aerospike.schema.scan`) to decide the name of bins/columns and their types. This implies that the derived schema depends entirely upon sampled records.  \n",
    "\n",
    "- Note that `__key` was not part of provided schema. So how can one query using `__key`? We can just add `__key` in provided schema with appropriate type. Similarly we can add `__gen` or `__ttl` etc.  \n",
    "         \n",
    "      val schemaWithPK: StructType = new StructType(Array(\n",
    "                StructField(\"__key\",IntegerType, nullable = false),    \n",
    "                StructField(\"id\", IntegerType, nullable = false),\n",
    "                StructField(\"name\", StringType, nullable = false),\n",
    "                StructField(\"age\", IntegerType, nullable = false),\n",
    "                StructField(\"salary\",IntegerType, nullable = false)))\n",
    "                \n",
    "- We recommend that you provide schema for queries that involve complex data types such as lists, maps, and mixed types. \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data without specifying any schema i.e. using connector schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- __key: string (nullable = true)\n",
      " |-- __digest: binary (nullable = false)\n",
      " |-- __expiry: integer (nullable = false)\n",
      " |-- __generation: integer (nullable = false)\n",
      " |-- __ttl: integer (nullable = false)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedDFWithoutSchema: org.apache.spark.sql.DataFrame = [__key: string, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Spark DataFrame by using the Connector Schema inference mechanism\n",
    "\n",
    "val loadedDFWithoutSchema=spark\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") //Path to feature file, while running in cluster this file needs to be on all drivers. Consult documentation on how to read from HDFS or as string. \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\") //read the data from this set\n",
    ".load\n",
    "loadedDFWithoutSchema.printSchema()\n",
    "//Notice that schema of loaded data has some additional fields. \n",
    "// When connector infers schema, it also adds internal metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data with user specified schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|829|name829| 29| 80292|\n",
      "|486|name486| 86| 59409|\n",
      "|759|name759| 59| 56312|\n",
      "|524|name524| 24| 69912|\n",
      "|215|name215| 15| 89198|\n",
      "+---+-------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedDFWithSchema: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Data can be loaded with known schema as well.\n",
    "val loadedDFWithSchema=spark\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".schema(schema)\n",
    ".option(\"aerospike.seedhost\",AS_HOST)\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\").load\n",
    "loadedDFWithSchema.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load nested data with user specified schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- aliases: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |-- last_name: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- home_address: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = true)\n",
      " |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |-- work_history: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = true)\n",
      " |    |    |-- company_address: struct (nullable = true)\n",
      " |    |    |    |-- zip: long (nullable = true)\n",
      " |    |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- worked_from: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "complex_data_json: String = resources/nested_data.json\n",
       "alias: org.apache.spark.sql.types.StructType = StructType(StructField(first_name,StringType,false), StructField(last_name,StringType,false))\n",
       "name: org.apache.spark.sql.types.StructType = StructType(StructField(first_name,StringType,false), StructField(aliases,ArrayType(StructType(StructField(first_name,StringType,false), StructField(last_name,StringType,false)),true),false))\n",
       "street_adress: org.apache.spark.sql.types.StructType = StructType(StructField(street_name,StringType,false), StructField(apt_number,IntegerType,false))\n",
       "address: org.apache.spark.sql.types.StructType = StructType(StructField(zip,LongType,false), StructField(street,StructType(StructField(street_name,StringType,false), StructField(apt_number,IntegerType,false)),fal..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val complex_data_json=\"resources/nested_data.json\"\n",
    "val alias=  StructType(List(\n",
    "    StructField(\"first_name\",StringType, false),\n",
    "    StructField(\"last_name\",StringType, false)))\n",
    "\n",
    "  val name= StructType(List(\n",
    "    StructField(\"first_name\",StringType, false),\n",
    "    StructField(\"aliases\",ArrayType(alias), false )\n",
    "  ))\n",
    "\n",
    "  val street_adress= StructType(List(\n",
    "    StructField(\"street_name\", StringType, false),\n",
    "    StructField(\"apt_number\" , IntegerType, false)))\n",
    "\n",
    "  val address = StructType( List(\n",
    "    StructField (\"zip\" , LongType, false),\n",
    "    StructField(\"street\", street_adress, false),\n",
    "    StructField(\"city\", StringType, false)))\n",
    "\n",
    "  val workHistory = StructType(List(\n",
    "    StructField (\"company_name\" , StringType, false),\n",
    "    StructField( \"company_address\" , address, false),\n",
    "    StructField(\"worked_from\", StringType, false)))\n",
    "\n",
    "  val person=  StructType ( List(\n",
    "    StructField(\"name\" , name, false, Metadata.empty),\n",
    "    StructField(\"SSN\", StringType, false,Metadata.empty),\n",
    "    StructField(\"home_address\", ArrayType(address), false),\n",
    "    StructField(\"work_history\", ArrayType(workHistory), false)))\n",
    "\n",
    "val cmplx_data_with_schema=spark.read.schema(person).json(complex_data_json)\n",
    "\n",
    "cmplx_data_with_schema.printSchema()\n",
    "cmplx_data_with_schema.write.mode(SaveMode.Overwrite) \n",
    ".format(\"com.aerospike.spark.sql\") //aerospike specific format\n",
    ".option(\"aerospike.seedhost\", AS_HOST) //db hostname, can be added multiple hosts, delimited with \":\"\n",
    ".option(\"aerospike.namespace\", \"test\") //use this namespace \n",
    ".option(\"aerospike.writeset\", \"scala_complex_input_data\") //write to this set\n",
    ".option(\"aerospike.updateByKey\", \"name.first_name\") //indicates which columns should be used for construction of primary key\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- __key: string (nullable = true)\n",
      " |-- __digest: binary (nullable = false)\n",
      " |-- __expiry: integer (nullable = false)\n",
      " |-- __generation: integer (nullable = false)\n",
      " |-- __ttl: integer (nullable = false)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- work_history: array (nullable = true)\n",
      " |    |-- element: binary (containsNull = true)\n",
      " |-- name: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- home_address: array (nullable = true)\n",
      " |    |-- element: binary (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedComplexDFWithoutSchema: org.apache.spark.sql.DataFrame = [__key: string, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val loadedComplexDFWithoutSchema=spark\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") //Path to feature file, while running in cluster this file needs to be on all drivers. Consult documentation on how to read from HDFS or as string. \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"scala_complex_input_data\") //read the data from this set\n",
    ".load\n",
    "loadedComplexDFWithoutSchema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- first_name: string (nullable = false)\n",
      " |    |-- aliases: array (nullable = false)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = false)\n",
      " |    |    |    |-- last_name: string (nullable = false)\n",
      " |-- SSN: string (nullable = false)\n",
      " |-- home_address: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = false)\n",
      " |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |-- city: string (nullable = false)\n",
      " |-- work_history: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = false)\n",
      " |    |    |-- company_address: struct (nullable = false)\n",
      " |    |    |    |-- zip: long (nullable = false)\n",
      " |    |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |    |-- city: string (nullable = false)\n",
      " |    |    |-- worked_from: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedComplexDFWithSchema: org.apache.spark.sql.DataFrame = [name: struct<first_name: string, aliases: array<struct<first_name:string,last_name:string>>>, SSN: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val loadedComplexDFWithSchema=spark\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") //Path to feature file, while running in cluster this file needs to be on all drivers. Consult documentation on how to read from HDFS or as string. \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"scala_complex_input_data\") //read the data from this set\n",
    ".schema(person)\n",
    ".load\n",
    "loadedComplexDFWithSchema.printSchema()\n",
    "//Please note the difference in types of loaded data in both cases. With schema, we extactly infer complex types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|                name|        SSN|        home_address|        work_history|           past_jobs|num_jobs|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|[Jamie, [[Patrici...|569-31-4715|[[53379, [James I...|[[Brown, Miller a...|[Brown, Miller an...|       5|\n",
      "|[Michael, [[Micha...|455-56-8642|[[2300, [Bauer Ov...|[[Harrington, All...|[Harrington, Alle...|       5|\n",
      "|[Luis, [[David, G...|818-16-1742|[[60659, [Oneill ...|[[Moss-Johnson, [...|[Moss-Johnson, St...|       5|\n",
      "|[Tami, [[Joseph, ...|001-49-0685|[[23288, [Clark V...|[[Roberts PLC, [4...|[Roberts PLC, Hub...|       5|\n",
      "|[Krista, [[Robert...|756-24-3462|[[64750, [Thomas ...|[[Baker PLC, [468...|[Baker PLC, Kirk ...|       5|\n",
      "|[Kristina, [[Vick...|545-62-3152|[[70288, [Rebecca...|[[Vaughn Inc, [20...|[Vaughn Inc, Brow...|       5|\n",
      "|[Elizabeth, [[And...|394-89-8545|[[45347, [Pierce ...|[[Gaines, Gray an...|[Gaines, Gray and...|       5|\n",
      "|[Bob, [[Theresa, ...|751-73-2267|[[25939, [Floyd H...|[[Cortez-Roberts,...|[Cortez-Roberts, ...|       5|\n",
      "|[Julie, [[Kyle, W...|845-58-5322|[[773, [Sarah Gre...|[[Kelly Group, [7...|[Kelly Group, Aus...|       5|\n",
      "|[Melissa, [[Erica...|002-25-1920|[[66571, [Derek C...|[[Bradford Ltd, [...|[Bradford Ltd, Gl...|       5|\n",
      "|[Sara, [[Ian, Bar...|749-22-5723|[[82017, [Coleman...|[[Anderson LLC, [...|[Anderson LLC, St...|       5|\n",
      "|[Christopher, [[J...|868-70-5021|[[13016, [Wood La...|[[Potter Inc, [60...|[Potter Inc, Pric...|       5|\n",
      "|[Samantha, [[Sele...|788-03-5996|[[60114, [Christo...|[[Haley, Barnett ...|[Haley, Barnett a...|       5|\n",
      "|[Christine, [[Gin...|411-04-6557|[[75021, [Shelia ...|[[Bass-Roth, [688...|[Bass-Roth, Torre...|       5|\n",
      "|[Fred, [[Andrew, ...|566-85-7824|[[91508, [Dawson ...|[[Harrell-Smith, ...|[Harrell-Smith, S...|       5|\n",
      "|[Jose, [[Ryan, Pu...|237-85-4119|[[51096, [Cassidy...|[[Armstrong and S...|[Armstrong and So...|       5|\n",
      "|[Laura, [[Mackenz...|145-75-4980|[[87038, [Isaiah ...|[[Williams, Thoma...|[Williams, Thomas...|       5|\n",
      "|[Beth, [[Julia, M...|745-90-4193|[[73086, [Thompso...|[[Allen, Smith an...|[Allen, Smith and...|       5|\n",
      "|[Robert, [[Jared,...|588-24-1668|[[28724, [Koch Is...|[[Anderson, Moon ...|[Anderson, Moon a...|       5|\n",
      "|[George, [[Stacy,...|368-19-0912|[[54976, [Angela ...|[[Smith-Petersen,...|[Smith-Petersen, ...|       5|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Find all people who have atleast 5 jobs in past.\n",
    "loadedComplexDFWithSchema\n",
    ".withColumn(\"past_jobs\", col(\"work_history.company_name\"))\n",
    ".withColumn(\"num_jobs\", size(col(\"past_jobs\")))\n",
    ".where(col(\"num_jobs\")  >4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering Aerospike Data using SparkSQL\n",
    "\n",
    "### Things to keep in mind\n",
    "   1. Queries that involve Primary Key in the predicate trigger aerospike_batch_get()( https://www.aerospike.com/docs/client/c/usage/kvs/batch.html) and run extremely fast. For e.g. a query containing `__key` with, with no `OR` between two bins.\n",
    "   2. All other queries may entail a scan of nodes if they can’t be converted to Aerospike batchget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that include Primary Key in the Predicate\n",
    "\n",
    "In case of batchget queries we can also apply filters upon metadata columns like `__gen` or `__ttl` etc. To do so, these columns should be exposed through schema (if schema provided). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|age|   name|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "|  829|[00 B0 3B 5E BD 9...|       0|          20|   -1| 29|name829| 80292|829|\n",
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batchGet1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchGet1= spark.sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when schema is not provided.\n",
    ".load.where(\"__key = 829\")\n",
    "batchGet1.show()\n",
    "//Please be aware Aerospike database supports only equality test with PKs in primary key query. \n",
    "//So, a where clause with \"__key >10\", would result in scan query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+---+------+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|age|  name|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+---+------+------+---+\n",
      "|   10|[16 50 E2 C7 BC 2...|       0|          20|   -1| 10|name10| 53572| 10|\n",
      "|   13|[9C 90 67 F0 7F E...|       0|          20|   -1| 13|name13| 76482| 13|\n",
      "|    7|[D3 C2 5B BE 77 3...|       0|          20|   -1|  7| name7| 98801|  7|\n",
      "|    9|[23 B3 1A E8 CB 0...|       0|          20|   -1|  9| name9| 74402|  9|\n",
      "|    6|[DD E4 52 09 AB 8...|       0|          20|   -1|  6| name6| 73131|  6|\n",
      "+-----+--------------------+--------+------------+-----+---+------+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "somePrimaryKeys: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "someMoreKeys: scala.collection.immutable.Range = Range(12, 13, 14)\n",
       "batchGet2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//In this query we are doing *OR* between PK subqueries \n",
    "\n",
    "val somePrimaryKeys= 1.to(10).toSeq\n",
    "val someMoreKeys= 12.to(14).toSeq\n",
    "val batchGet2= spark.sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\",AS_HOST)\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when inferred without schema.\n",
    ".load.where((col(\"__key\") isin (somePrimaryKeys:_*)) || ( col(\"__key\") isin (someMoreKeys:_*) ))\n",
    "batchGet2.show(5)\n",
    "//We should got in total 13 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that do not include Primary Key in the Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|age|   name|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "| null|[0A 60 1D 97 98 5...|       0|          20|   -1| 86|name486| 59409|486|\n",
      "| null|[0D 60 A3 4C 0C C...|       0|          20|   -1| 59|name759| 56312|759|\n",
      "| null|[14 40 BB E5 AC F...|       0|          20|   -1| 96|name796| 73910|796|\n",
      "| null|[15 70 45 1B 30 7...|       0|          20|   -1| 54|name654| 98743|654|\n",
      "| null|[15 10 82 D6 73 0...|       0|          20|   -1| 61|name961| 69617|961|\n",
      "| null|[29 90 F4 A2 39 9...|       0|          20|   -1| 64| name64| 68774| 64|\n",
      "| null|[57 00 4F E5 E7 D...|       0|          20|   -1| 93|name793| 97343|793|\n",
      "| null|[59 A0 08 B1 16 1...|       0|          20|   -1| 74|name474| 76572|474|\n",
      "| null|[5D F0 D3 FE E9 1...|       0|          20|   -1| 54|name454| 72084|454|\n",
      "| null|[6E F0 D2 3A B3 4...|       0|          20|   -1| 73|name273| 71142|273|\n",
      "| null|[82 30 70 1D 7A E...|       0|          20|   -1| 85|name285| 50810|285|\n",
      "| null|[8A 00 09 F6 2F 5...|       0|          20|   -1| 76|name476| 68412|476|\n",
      "| null|[8B 70 F1 B2 E4 0...|       0|          20|   -1| 76|name376| 99872|376|\n",
      "| null|[9E 70 31 75 C2 4...|       0|          20|   -1| 61|name561| 62093|561|\n",
      "| null|[AF 50 95 32 00 E...|       0|          20|   -1| 51|name551| 54507|551|\n",
      "| null|[AF 40 52 C7 50 3...|       0|          20|   -1| 79|name179| 97719|179|\n",
      "| null|[B2 90 83 2A D5 6...|       0|          20|   -1| 79|name679| 99702|679|\n",
      "| null|[B2 80 BF 17 88 0...|       0|          20|   -1| 80|name380| 59231|380|\n",
      "| null|[B4 D0 3F 3B 8A 3...|       0|          20|   -1| 82|name382| 99669|382|\n",
      "| null|[B5 30 84 49 68 E...|       0|          20|   -1| 91|name791| 75984|791|\n",
      "+-----+--------------------+--------+------------+-----+---+-------+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "somePrimaryKeys: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "scanQuery1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val somePrimaryKeys= 1.to(10).toSeq\n",
    "val scanQuery1= spark.sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option(\"aerospike.set\", \"input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when inferred without schema.\n",
    ".load.where((col(\"__key\") isin (somePrimaryKeys:_*)) || ( col(\"age\") >50 ))\n",
    "\n",
    "scanQuery1.show()\n",
    "\n",
    "//Since there is OR between PKs and Bin. It will be treated as Scan query. \n",
    "//Primary keys are not stored in bins(by default), hence only filters corresponding to bins are honored.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
