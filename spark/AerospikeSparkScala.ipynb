{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aerospike Spark Connector Tutorial for Scala\n",
    "\n",
    "## Tested with Java 8, Spark 2.4.0, Python 3.7,  Scala 2.11.12, and  Spylon ( https://pypi.org/project/spylon-kernel/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.jars = [\"aerospike-spark-assembly-2.5.0.jar\"] \n",
    "launcher.master = \"local[*]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.3:4040\n",
       "SparkContext available as 'sc' (version = 2.4.6, master = local[*], app id = local-1602545132812)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AS_HOST: String = 172.16.39.219:3000\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Specify the Seed Host of the Aerospike Server\n",
    "val AS_HOST =\"127.0.0.1:3000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable.ArrayBuffer\n",
       "import org.apache.spark.sql.Row\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.SaveMode\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SaveMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema in the Spark Connector\n",
    "\n",
    "-  Aerospike is schemaless, however spark adher to schema. After the schema is decided upon (either through inference or given), data within the bins must honor the types. \n",
    "\n",
    "- To infer the schema, the connector samples a set of records (configurable through `aerospike.schema.scan`) to decide the name of bins/columns and their types. This implies that the derived schema depends entirely upon sampled records.  \n",
    "\n",
    "- **Note that `__key` was not part of provided schema. So how can one query using `__key`? We can just add `__key` in provided schema with appropriate type. Similarly we can add `__gen` or `__ttl` etc.**  \n",
    "         \n",
    "      val schemaWithPK: StructType = new StructType(Array(\n",
    "                StructField(\"__key\",IntegerType, nullable = false),    \n",
    "                StructField(\"id\", IntegerType, nullable = false),\n",
    "                StructField(\"name\", StringType, nullable = false),\n",
    "                StructField(\"age\", IntegerType, nullable = false),\n",
    "                StructField(\"salary\",IntegerType, nullable = false)))\n",
    "                \n",
    "- **We recommend that you provide schema for queries that involve complex data types such as lists, maps, and mixed types. Using schema inference for CDT may cause unexpected issues.** \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexible schema inference \n",
    "\n",
    "- Spark assumes that the underlying data store (Aerospike in this case) follows a strict schema for all the records within a table. However, Aerospike is a No-SQL DB and is schemaless. Hence a single bin (mapped to a column ) within a set ( mapped to a table ) could technically hold values of multiple Aerospike supported types. The Spark connector reconciles this incompatibility with help of certain rules. Please choose the configuration that suits your use case. The strict configuration (aerospike.schema.flexible = false ) could be used when you have modeled your data in Aerospike to adhere to a strict schema i.e. each record within the set has the same schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aerospike.schema.flexible = true (default) \n",
    "   \n",
    "   -  If none of the column types in the user-specified schema match the bin types of a record in Aerospike, a record with NULLs is returned in the result set. \n",
    "  -  Please use the filter() in Spark to filter out NULL records. For e.g. df.filter(\"gender == NULL\").show(false), where df is a dataframe and gender is a field that was not specified in the user-specified schema. \n",
    "\n",
    "  - If the above mismatch is limited to fewer columns in the user-specified schema then NULL would be returned for those columns in the result set. **Note: there is no way to tell apart a NULL due to missing value in the original data set and the NULL due to mismatch, at this point. Hence, the user would have to treat all NULLs as missing values.** The columns that are not a part of the schema will be automatically filtered out in the result set by the connector.\n",
    "\n",
    "  - Please note that if any field is set to NOT nullable i.e. nullable = false, your query will error out if there’s a type mismatch between an Aerospike bin and the column type specified in the user-specified schema.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample data to demonstrate flexible schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|one| two|\n",
      "+---+----+\n",
      "| 82|  82|\n",
      "| 67|null|\n",
      "| 29|null|\n",
      "| 39|null|\n",
      "| 16|  16|\n",
      "| 34|  34|\n",
      "|  1|null|\n",
      "| 77|null|\n",
      "| 52|  52|\n",
      "| 27|null|\n",
      "| 25|null|\n",
      "| 11|null|\n",
      "| 15|null|\n",
      "| 96|  96|\n",
      "| 97|null|\n",
      "|  4|   4|\n",
      "| 89|null|\n",
      "| 14|  14|\n",
      "| 79|null|\n",
      "| 71|null|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import com.aerospike.client.policy.WritePolicy\n",
       "import com.aerospike.spark.sql.AerospikeConnection\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import com.aerospike.client.{AerospikeClient, AerospikeException, Bin, Key}\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@cbb0c0c\n",
       "spark1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@60309276\n",
       "client: com.aerospike.client.AerospikeClient = com.aerospike.client.AerospikeClient@7d2f16a7\n",
       "flexsetname: String = flexschema\n",
       "wp: com.aerospike.client.policy.WritePolicy = com.aerospike.client.policy.WritePolicy@811e5be\n",
       "wp.expiration: Int = 600\n",
       "flexibleSchema: org.apache.spark.sql.types.StructType = StructType(StructField(one,IntegerType,true), StructField(two,IntegerType,true))\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import com.aerospike.client.policy.WritePolicy\n",
    "import com.aerospike.spark.sql.AerospikeConnection\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import com.aerospike.client.{AerospikeClient, AerospikeException, Bin, Key}\n",
    "val conf = sc.getConf.clone();\n",
    "conf.set(\"aerospike.seedhost\" , AS_HOST)\n",
    "conf.set(\"aerospike.schema.flexible\" , \"true\") //by default it is always true\n",
    "val spark1= SparkSession.builder().config(conf).master(\"local[2]\").getOrCreate()\n",
    "val client = AerospikeConnection.getClient(conf)\n",
    "val flexsetname = \"flexschema\"\n",
    "val wp = new WritePolicy()\n",
    "    wp.expiration = 600 // expire data in 10 minutes\n",
    "    for (i <- 1 to 100) {\n",
    "      val key = new Key(\"test\", setname, i)\n",
    "      client.delete(null, key )\n",
    "      if( i %2 ==0){\n",
    "        client.put(wp, key, new Bin(\"one\", i.toInt), new Bin(\"two\", i.toInt))\n",
    "      }else{\n",
    "        client.put(wp, key, new Bin(\"one\", i.toInt), new Bin(\"two\", i.toString))\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "val flexibleSchema= StructType (\n",
    "      Seq(\n",
    "        StructField(\"one\", IntegerType, true ),\n",
    "        StructField(\"two\", IntegerType, true )\n",
    "      )\n",
    "    )\n",
    "spark1.sqlContext.read.format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") //Path to feature file, while running in cluster this file needs to be on all drivers. Consult documentation on how to read from HDFS or as string. \n",
    ".option(\"aerospike.set\", flexsetname)\n",
    ".option(\"aerospike.namespace\", \"test\")\n",
    ".schema(flexibleSchema).load().show()\n",
    "//Please note that, in case of type mismatch all columns with odd value of `one`(which had string type) is set to null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aerospike.schema.flexible = false \n",
    "\n",
    "   - If a mismatch between the user-specified schema and the schema of a record in Aerospike is detected at the bin/column level, your query will error out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 6 in stage 38.0 failed 1 times, most recent failure: Lost task 6.0 in stage 38.0 (TID 2061, localhost, executor driver): com.aerospike.spark.sql.TypeConverter$TypeMismatchException",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 38.0 failed 1 times, most recent failure: Lost task 6.0 in stage 38.0 (TID 2061, localhost, executor driver): com.aerospike.spark.sql.TypeConverter$TypeMismatchException",
      "\tat com.aerospike.spark.sql.TypeConverter$.matchesSchemaType$1(TypeConverter.scala:158)",
      "\tat com.aerospike.spark.sql.TypeConverter$.binToValue(TypeConverter.scala:190)",
      "\tat com.aerospike.spark.sql.RowIterator$$anonfun$29.apply(KeyRecordRDD.scala:701)",
      "\tat com.aerospike.spark.sql.RowIterator$$anonfun$29.apply(KeyRecordRDD.scala:694)",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "\tat com.aerospike.spark.sql.RowIterator.next(KeyRecordRDD.scala:694)",
      "\tat com.aerospike.spark.sql.RowIterator.next(KeyRecordRDD.scala:667)",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)",
      "  at scala.Option.foreach(Option.scala:257)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)",
      "  ... 44 elided",
      "Caused by: com.aerospike.spark.sql.TypeConverter$TypeMismatchException",
      "  at com.aerospike.spark.sql.TypeConverter$.matchesSchemaType$1(TypeConverter.scala:158)",
      "  at com.aerospike.spark.sql.TypeConverter$.binToValue(TypeConverter.scala:190)",
      "  at com.aerospike.spark.sql.RowIterator$$anonfun$29.apply(KeyRecordRDD.scala:701)",
      "  at com.aerospike.spark.sql.RowIterator$$anonfun$29.apply(KeyRecordRDD.scala:694)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at com.aerospike.spark.sql.RowIterator.next(KeyRecordRDD.scala:694)",
      "  at com.aerospike.spark.sql.RowIterator.next(KeyRecordRDD.scala:667)",
      "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "//When strict matching is set, we will get an exception due to type mismatch with schema provided.\n",
    "val strictConf = conf.clone();\n",
    "strictConf.set(\"aerospike.schema.flexible\" , \"false\")\n",
    "val spark2 = SparkSession.builder().config(strictConf).master(\"local[2]\").getOrCreate()\n",
    "val df = spark2.\n",
    "sqlContext.\n",
    "read.\n",
    "format(\"com.aerospike.spark.sql\").\n",
    "option(\"aerospike.seedhost\", AS_HOST).\n",
    "option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\").\n",
    "option(\"aerospike.set\", flexsetname).\n",
    "option(\"aerospike.namespace\", \"test\").\n",
    "schema(flexibleSchema)\n",
    ".load()\n",
    ".show()\n",
    "spark2.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample data and write it into Aerospike Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------+\n",
      "| id|  name|age|salary|\n",
      "+---+------+---+------+\n",
      "|  1| name1|  1| 52885|\n",
      "|  2| name2|  2| 93130|\n",
      "|  3| name3|  3| 59812|\n",
      "|  4| name4|  4| 68815|\n",
      "|  5| name5|  5| 84694|\n",
      "|  6| name6|  6| 99282|\n",
      "|  7| name7|  7| 87950|\n",
      "|  8| name8|  8| 91397|\n",
      "|  9| name9|  9| 94050|\n",
      "| 10|name10| 10| 94981|\n",
      "+---+------+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num_records: Int = 1000\n",
       "rand: util.Random.type = scala.util.Random$@43d261c1\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(name,StringType,false), StructField(age,IntegerType,false), StructField(salary,IntegerType,false))\n",
       "inputDF: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Create test data\n",
    "\n",
    "val num_records=1000\n",
    "val rand = scala.util.Random\n",
    "\n",
    "//schema of input data\n",
    "\n",
    "val schema: StructType = new StructType(\n",
    "    Array(\n",
    "    StructField(\"id\", IntegerType, nullable = false),\n",
    "    StructField(\"name\", StringType, nullable = false),\n",
    "    StructField(\"age\", IntegerType, nullable = false),\n",
    "    StructField(\"salary\",IntegerType, nullable = false)\n",
    "  ))\n",
    "\n",
    "val inputDF = {\n",
    "    val inputBuf=  new ArrayBuffer[Row]()\n",
    "    for ( i <- 1 to num_records){\n",
    "        val name = \"name\"  + i\n",
    "        val age = i%100\n",
    "        val salary = 50000 + rand.nextInt(50000)\n",
    "        val id = i \n",
    "        val r = Row(id, name, age,salary)\n",
    "        inputBuf.append(r)\n",
    "    }\n",
    "    val inputRDD = spark.sparkContext.parallelize(inputBuf.toSeq)\n",
    "    spark.createDataFrame(inputRDD,schema)\n",
    "}\n",
    "\n",
    "inputDF.show(10)\n",
    "\n",
    "//Write the Sample Data to Aerospike\n",
    "inputDF.write.mode(SaveMode.Overwrite) \n",
    ".format(\"com.aerospike.spark.sql\") //aerospike specific format\n",
    ".option(\"aerospike.seedhost\", AS_HOST) //db hostname, can be added multiple hosts, delimited with \":\"\n",
    ".option(\"aerospike.namespace\", \"test\") //use this namespace \n",
    ".option(\"aerospike.writeset\", \"input_data\") //write to this set\n",
    ".option(\"aerospike.updateByKey\", \"id\") //indicates which columns should be used for construction of primary key\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame without specifying any schema i.e. using connector schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- __key: string (nullable = true)\n",
      " |-- __digest: binary (nullable = false)\n",
      " |-- __expiry: integer (nullable = false)\n",
      " |-- __generation: integer (nullable = false)\n",
      " |-- __ttl: integer (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedDFWithoutSchema: org.apache.spark.sql.DataFrame = [__key: string, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Spark DataFrame by using the Connector Schema inference mechanism\n",
    "\n",
    "val loadedDFWithoutSchema=spark\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") //Path to feature file, while running in cluster this file needs to be on all drivers. Consult documentation on how to read from HDFS or as string. \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\") //read the data from this set\n",
    ".load\n",
    "loadedDFWithoutSchema.printSchema()\n",
    "//Notice that schema of loaded data has some additional fields. \n",
    "// When connector infers schema, it also adds internal metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a DataFrame with user specified schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|829|name829| 29| 57478|\n",
      "|486|name486| 86| 87062|\n",
      "|759|name759| 59| 96292|\n",
      "|524|name524| 24| 84900|\n",
      "|215|name215| 15| 68587|\n",
      "+---+-------+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedDFWithSchema: org.apache.spark.sql.DataFrame = [id: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Data can be loaded with known schema as well.\n",
    "val loadedDFWithSchema=spark\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".schema(schema)\n",
    ".option(\"aerospike.seedhost\",AS_HOST)\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\").load\n",
    "loadedDFWithSchema.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Sample Complex Data Types (CDT) data into Aerospike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- aliases: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |    |-- last_name: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- home_address: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = true)\n",
      " |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |-- work_history: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = true)\n",
      " |    |    |-- company_address: struct (nullable = true)\n",
      " |    |    |    |-- zip: long (nullable = true)\n",
      " |    |    |    |-- street: struct (nullable = true)\n",
      " |    |    |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- worked_from: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "complex_data_json: String = resources/nested_data.json\n",
       "alias: org.apache.spark.sql.types.StructType = StructType(StructField(first_name,StringType,false), StructField(last_name,StringType,false))\n",
       "name: org.apache.spark.sql.types.StructType = StructType(StructField(first_name,StringType,false), StructField(aliases,ArrayType(StructType(StructField(first_name,StringType,false), StructField(last_name,StringType,false)),true),false))\n",
       "street_adress: org.apache.spark.sql.types.StructType = StructType(StructField(street_name,StringType,false), StructField(apt_number,IntegerType,false))\n",
       "address: org.apache.spark.sql.types.StructType = StructType(StructField(zip,LongType,false), StructField(street,StructType(StructField(street_name,StringType,false), StructField(apt_number,IntegerType,false)),fal..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val complex_data_json=\"resources/nested_data.json\"\n",
    "val alias=  StructType(List(\n",
    "    StructField(\"first_name\",StringType, false),\n",
    "    StructField(\"last_name\",StringType, false)))\n",
    "\n",
    "  val name= StructType(List(\n",
    "    StructField(\"first_name\",StringType, false),\n",
    "    StructField(\"aliases\",ArrayType(alias), false )\n",
    "  ))\n",
    "\n",
    "  val street_adress= StructType(List(\n",
    "    StructField(\"street_name\", StringType, false),\n",
    "    StructField(\"apt_number\" , IntegerType, false)))\n",
    "\n",
    "  val address = StructType( List(\n",
    "    StructField (\"zip\" , LongType, false),\n",
    "    StructField(\"street\", street_adress, false),\n",
    "    StructField(\"city\", StringType, false)))\n",
    "\n",
    "  val workHistory = StructType(List(\n",
    "    StructField (\"company_name\" , StringType, false),\n",
    "    StructField( \"company_address\" , address, false),\n",
    "    StructField(\"worked_from\", StringType, false)))\n",
    "\n",
    "  val person=  StructType ( List(\n",
    "    StructField(\"name\" , name, false, Metadata.empty),\n",
    "    StructField(\"SSN\", StringType, false,Metadata.empty),\n",
    "    StructField(\"home_address\", ArrayType(address), false),\n",
    "    StructField(\"work_history\", ArrayType(workHistory), false)))\n",
    "\n",
    "val cmplx_data_with_schema=spark.read.schema(person).json(complex_data_json)\n",
    "\n",
    "cmplx_data_with_schema.printSchema()\n",
    "cmplx_data_with_schema.write.mode(SaveMode.Overwrite) \n",
    ".format(\"com.aerospike.spark.sql\") //aerospike specific format\n",
    ".option(\"aerospike.seedhost\", AS_HOST) //db hostname, can be added multiple hosts, delimited with \":\"\n",
    ".option(\"aerospike.namespace\", \"test\") //use this namespace \n",
    ".option(\"aerospike.writeset\", \"scala_complex_input_data\") //write to this set\n",
    ".option(\"aerospike.updateByKey\", \"name.first_name\") //indicates which columns should be used for construction of primary key\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Complex Data Types (CDT) into a DataFrame with user specified schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- first_name: string (nullable = false)\n",
      " |    |-- aliases: array (nullable = false)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- first_name: string (nullable = false)\n",
      " |    |    |    |-- last_name: string (nullable = false)\n",
      " |-- SSN: string (nullable = false)\n",
      " |-- home_address: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- zip: long (nullable = false)\n",
      " |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |-- city: string (nullable = false)\n",
      " |-- work_history: array (nullable = false)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- company_name: string (nullable = false)\n",
      " |    |    |-- company_address: struct (nullable = false)\n",
      " |    |    |    |-- zip: long (nullable = false)\n",
      " |    |    |    |-- street: struct (nullable = false)\n",
      " |    |    |    |    |-- street_name: string (nullable = false)\n",
      " |    |    |    |    |-- apt_number: integer (nullable = false)\n",
      " |    |    |    |-- city: string (nullable = false)\n",
      " |    |    |-- worked_from: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "loadedComplexDFWithSchema: org.apache.spark.sql.DataFrame = [name: struct<first_name: string, aliases: array<struct<first_name:string,last_name:string>>>, SSN: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val loadedComplexDFWithSchema=spark\n",
    ".sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.keyPath\", \"/etc/aerospike/features.conf\") //Path to feature file, while running in cluster this file needs to be on all drivers. Consult documentation on how to read from HDFS or as string. \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"scala_complex_input_data\") //read the data from this set\n",
    ".schema(person)\n",
    ".load\n",
    "loadedComplexDFWithSchema.printSchema()\n",
    "//Please note the difference in types of loaded data in both cases. With schema, we extactly infer complex types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quering Aerospike Data using SparkSQL\n",
    "\n",
    "### Things to keep in mind\n",
    "   1. Queries that involve Primary Key in the predicate trigger aerospike_batch_get()( https://www.aerospike.com/docs/client/c/usage/kvs/batch.html) and run extremely fast. For e.g. a query containing `__key` with, with no `OR` between two bins.\n",
    "   2. All other queries may entail a full scan of the Aerospike DB if they can’t be converted to Aerospike batchget. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that include Primary Key in the Predicate\n",
    "\n",
    "In case of batchget queries we can also apply filters upon metadata columns like `__gen` or `__ttl` etc. To do so, these columns should be exposed through schema (if schema provided). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|   name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|  829|[00 B0 3B 5E BD 9...|       0|           1|   -1|name829| 29| 57478|829|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "batchGet1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val batchGet1= spark.sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when schema is not provided.\n",
    ".load.where(\"__key = 829\")\n",
    "batchGet1.show()\n",
    "//Please be aware Aerospike database supports only equality test with PKs in primary key query. \n",
    "//So, a where clause with \"__key >10\", would result in scan query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|  name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+------+---+------+---+\n",
      "|   10|[16 50 E2 C7 BC 2...|       0|           1|   -1|name10| 10| 94981| 10|\n",
      "|   13|[9C 90 67 F0 7F E...|       0|           1|   -1|name13| 13| 59845| 13|\n",
      "|    7|[D3 C2 5B BE 77 3...|       0|           1|   -1| name7|  7| 87950|  7|\n",
      "|    9|[23 B3 1A E8 CB 0...|       0|           1|   -1| name9|  9| 94050|  9|\n",
      "|    6|[DD E4 52 09 AB 8...|       0|           1|   -1| name6|  6| 99282|  6|\n",
      "+-----+--------------------+--------+------------+-----+------+---+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "somePrimaryKeys: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "someMoreKeys: scala.collection.immutable.Range = Range(12, 13, 14)\n",
       "batchGet2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//In this query we are doing *OR* between PK subqueries \n",
    "\n",
    "val somePrimaryKeys= 1.to(10).toSeq\n",
    "val someMoreKeys= 12.to(14).toSeq\n",
    "val batchGet2= spark.sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\",AS_HOST)\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.set\", \"input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when inferred without schema.\n",
    ".load.where((col(\"__key\") isin (somePrimaryKeys:_*)) || ( col(\"__key\") isin (someMoreKeys:_*) ))\n",
    "batchGet2.show(5)\n",
    "//We should got in total 13 records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries that do not include Primary Key in the Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "|__key|            __digest|__expiry|__generation|__ttl|   name|age|salary| id|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "| null|[0A 60 1D 97 98 5...|       0|           1|   -1|name486| 86| 87062|486|\n",
      "| null|[0D 60 A3 4C 0C C...|       0|           1|   -1|name759| 59| 96292|759|\n",
      "| null|[14 40 BB E5 AC F...|       0|           1|   -1|name796| 96| 56004|796|\n",
      "| null|[15 70 45 1B 30 7...|       0|           1|   -1|name654| 54| 89596|654|\n",
      "| null|[15 10 82 D6 73 0...|       0|           1|   -1|name961| 61| 57855|961|\n",
      "| null|[29 90 F4 A2 39 9...|       0|           1|   -1| name64| 64| 50306| 64|\n",
      "| null|[57 00 4F E5 E7 D...|       0|           1|   -1|name793| 93| 67661|793|\n",
      "| null|[59 A0 08 B1 16 1...|       0|           1|   -1|name474| 74| 80470|474|\n",
      "| null|[5D F0 D3 FE E9 1...|       0|           1|   -1|name454| 54| 89597|454|\n",
      "| null|[6E F0 D2 3A B3 4...|       0|           1|   -1|name273| 73| 76483|273|\n",
      "| null|[82 30 70 1D 7A E...|       0|           1|   -1|name285| 85| 66645|285|\n",
      "| null|[8A 00 09 F6 2F 5...|       0|           1|   -1|name476| 76| 55360|476|\n",
      "| null|[8B 70 F1 B2 E4 0...|       0|           1|   -1|name376| 76| 64447|376|\n",
      "| null|[9E 70 31 75 C2 4...|       0|           1|   -1|name561| 61| 90305|561|\n",
      "| null|[AF 50 95 32 00 E...|       0|           1|   -1|name551| 51| 73429|551|\n",
      "| null|[AF 40 52 C7 50 3...|       0|           1|   -1|name179| 79| 61836|179|\n",
      "| null|[B2 90 83 2A D5 6...|       0|           1|   -1|name679| 79| 58606|679|\n",
      "| null|[B2 80 BF 17 88 0...|       0|           1|   -1|name380| 80| 84918|380|\n",
      "| null|[B4 D0 3F 3B 8A 3...|       0|           1|   -1|name382| 82| 91706|382|\n",
      "| null|[B5 30 84 49 68 E...|       0|           1|   -1|name791| 91| 98608|791|\n",
      "+-----+--------------------+--------+------------+-----+-------+---+------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "somePrimaryKeys: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
       "scanQuery1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [__key: int, __digest: binary ... 7 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val somePrimaryKeys= 1.to(10).toSeq\n",
    "val scanQuery1= spark.sqlContext\n",
    ".read\n",
    ".format(\"com.aerospike.spark.sql\")\n",
    ".option(\"aerospike.seedhost\", AS_HOST)\n",
    ".option (\"aerospike.namespace\", \"test\")\n",
    ".option(\"aerospike.featurekey\", \"/etc/aerospike/features.conf\") \n",
    ".option(\"aerospike.set\", \"input_data\")\n",
    ".option(\"aerospike.keyType\", \"int\") //used to hint primary key(PK) type when inferred without schema.\n",
    ".load.where((col(\"__key\") isin (somePrimaryKeys:_*)) || ( col(\"age\") >50 ))\n",
    "\n",
    "scanQuery1.show()\n",
    "\n",
    "//Since there is OR between PKs and Bin. It will be treated as Scan query. \n",
    "//Primary keys are not stored in bins(by default), hence only filters corresponding to bins are honored.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query with CDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|                name|        SSN|        home_address|        work_history|           past_jobs|num_jobs|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "|[Jamie, [[Patrici...|569-31-4715|[[53379, [James I...|[[Brown, Miller a...|[Brown, Miller an...|       5|\n",
      "|[Michael, [[Micha...|455-56-8642|[[2300, [Bauer Ov...|[[Harrington, All...|[Harrington, Alle...|       5|\n",
      "|[Luis, [[David, G...|818-16-1742|[[60659, [Oneill ...|[[Moss-Johnson, [...|[Moss-Johnson, St...|       5|\n",
      "|[Tami, [[Joseph, ...|001-49-0685|[[23288, [Clark V...|[[Roberts PLC, [4...|[Roberts PLC, Hub...|       5|\n",
      "|[Krista, [[Robert...|756-24-3462|[[64750, [Thomas ...|[[Baker PLC, [468...|[Baker PLC, Kirk ...|       5|\n",
      "|[Kristina, [[Vick...|545-62-3152|[[70288, [Rebecca...|[[Vaughn Inc, [20...|[Vaughn Inc, Brow...|       5|\n",
      "|[Elizabeth, [[And...|394-89-8545|[[45347, [Pierce ...|[[Gaines, Gray an...|[Gaines, Gray and...|       5|\n",
      "|[Bob, [[Theresa, ...|751-73-2267|[[25939, [Floyd H...|[[Cortez-Roberts,...|[Cortez-Roberts, ...|       5|\n",
      "|[Julie, [[Kyle, W...|845-58-5322|[[773, [Sarah Gre...|[[Kelly Group, [7...|[Kelly Group, Aus...|       5|\n",
      "|[Melissa, [[Erica...|002-25-1920|[[66571, [Derek C...|[[Bradford Ltd, [...|[Bradford Ltd, Gl...|       5|\n",
      "|[Sara, [[Ian, Bar...|749-22-5723|[[82017, [Coleman...|[[Anderson LLC, [...|[Anderson LLC, St...|       5|\n",
      "|[Christopher, [[J...|868-70-5021|[[13016, [Wood La...|[[Potter Inc, [60...|[Potter Inc, Pric...|       5|\n",
      "|[Samantha, [[Sele...|788-03-5996|[[60114, [Christo...|[[Haley, Barnett ...|[Haley, Barnett a...|       5|\n",
      "|[Christine, [[Gin...|411-04-6557|[[75021, [Shelia ...|[[Bass-Roth, [688...|[Bass-Roth, Torre...|       5|\n",
      "|[Fred, [[Andrew, ...|566-85-7824|[[91508, [Dawson ...|[[Harrell-Smith, ...|[Harrell-Smith, S...|       5|\n",
      "|[Jose, [[Ryan, Pu...|237-85-4119|[[51096, [Cassidy...|[[Armstrong and S...|[Armstrong and So...|       5|\n",
      "|[Laura, [[Mackenz...|145-75-4980|[[87038, [Isaiah ...|[[Williams, Thoma...|[Williams, Thomas...|       5|\n",
      "|[Beth, [[Julia, M...|745-90-4193|[[73086, [Thompso...|[[Allen, Smith an...|[Allen, Smith and...|       5|\n",
      "|[Robert, [[Jared,...|588-24-1668|[[28724, [Koch Is...|[[Anderson, Moon ...|[Anderson, Moon a...|       5|\n",
      "|[George, [[Stacy,...|368-19-0912|[[54976, [Angela ...|[[Smith-Petersen,...|[Smith-Petersen, ...|       5|\n",
      "+--------------------+-----------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Find all people who have atleast 5 jobs in past.\n",
    "loadedComplexDFWithSchema\n",
    ".withColumn(\"past_jobs\", col(\"work_history.company_name\"))\n",
    ".withColumn(\"num_jobs\", size(col(\"past_jobs\")))\n",
    ".where(col(\"num_jobs\")  >4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Aerospike Spark Connector Configuration properties in the Spark API to improve performance\n",
    "\n",
    "aerospike.partition.factor: number of logical aerospike partitions [0-15]\n",
    "aerospike.maxthreadcount : maximum number of threads to use for writing data into Aerospike\n",
    "aerospike.compression : compression of java client-server communication\n",
    "aerospike.batchMax : maximum number of records per read request (default 5000)\n",
    "aerospike.recordspersecond : same as java client\n",
    "\n",
    "#### Other\n",
    "aerospike.keyType : Primary key type hint for schema inference. Always set it properly if primary key type is not string\n",
    "\n",
    "See https://www.aerospike.com/docs/connect/processing/spark/reference.html for detailed description of the above properties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
